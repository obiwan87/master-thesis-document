@misc{globalpulse,
  author = {Global Pulse},
  title = {{Global Pulse Projects}},
  howpublished = "\url{http://www.unglobalpulse.org/projects}",
  year = {2017}, 
  note = "[Online; accessed 02-June-2017]"
}

@misc{googlefludengue,
  author = {Google},
  title = {{Google Flu Trends, Google Dengue Trends}},
  howpublished = "\url{https://www.google.org/flutrends/about/}",
  year = {2017}, 
  note = "[Online; accessed 03-June-2017]"
}

@article{sebastiani2002machine,
  title={Machine learning in automated text categorization},
  author={Sebastiani, Fabrizio},
  journal={ACM computing surveys (CSUR)},
  volume={34},
  number={1},
  pages={1--47},
  year={2002},
  publisher={ACM}
}


@inproceedings{baker1998distributional,
  title={Distributional clustering of words for text classification},
  author={Baker, L Douglas and McCallum, Andrew Kachites},
  booktitle={Proceedings of the 21st annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={96--103},
  year={1998},
  organization={ACM}
}

@inproceedings{yang1997comparative,
  title={A comparative study on feature selection in text categorization},
  author={Yang, Yiming and Pedersen, Jan O},
  booktitle={Icml},
  volume={97},
  pages={412--420},
  year={1997}
}

@article{singh2010feature,
  title={Feature Selection for Text Classification Based on Gini Coefficient of Inequality.},
  author={Singh, Sanasam Ranbir and Murthy, Hema A and Gonsalves, Timothy A},
  journal={Fsdm},
  volume={10},
  pages={76--85},
  year={2010}
}

@inproceedings{mccallum1998comparison,
  title={A comparison of event models for naive bayes text classification},
  author={McCallum, Andrew and Nigam, Kamal and others},
  booktitle={AAAI-98 workshop on learning for text categorization},
  volume={752},
  pages={41--48},
  year={1998},
  organization={Madison, WI}
}

@inproceedings{wang2012baselines,
  title={Baselines and bigrams: Simple, good sentiment and topic classification},
  author={Wang, Sida and Manning, Christopher D},
  booktitle={Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics: Short Papers-Volume 2},
  pages={90--94},
  year={2012},
  organization={Association for Computational Linguistics}
}

@inproceedings{slonim2001power,
  title={The power of word clusters for text classification},
  author={Slonim, Noam and Tishby, Naftali},
  booktitle={23rd European Colloquium on Information Retrieval Research},
  volume={1},
  pages={200},
  year={2001}
}

@inproceedings{ma2015using,
  title={Using Word2Vec to process big text data},
  author={Ma, Long and Zhang, Yanqing},
  booktitle={Big Data (Big Data), 2015 IEEE International Conference on},
  pages={2895--2897},
  year={2015},
  organization={IEEE}
}

@inproceedings{mikolov2013distributed,
  title={Distributed representations of words and phrases and their compositionality},
  author={Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
  booktitle={Advances in neural information processing systems},
  pages={3111--3119},
  year={2013}
}

@article{wang2016semantic,
  title={Semantic expansion using word embedding clustering and convolutional neural network for improving short text classification},
  author={Wang, Peng and Xu, Bo and Xu, Jiaming and Tian, Guanhua and Liu, Cheng-Lin and Hao, Hongwei},
  journal={Neurocomputing},
  volume={174},
  pages={806--814},
  year={2016},
  publisher={Elsevier}
}

@inproceedings{le2014distributed,
  title={Distributed representations of sentences and documents},
  author={Le, Quoc and Mikolov, Tomas},
  booktitle={Proceedings of the 31st International Conference on Machine Learning (ICML-14)},
  pages={1188--1196},
  year={2014}
}

@article{zhang2004optimality,
  title={The optimality of naive Bayes},
  author={Zhang, Harry},
  journal={AA},
  volume={1},
  number={2},
  pages={3},
  year={2004}
}

@article{furnkranz1998study,
  title={A study using n-gram features for text categorization},
  author={Fürnkranz, Johannes},
  journal={Austrian Research Institute for Artifical Intelligence},
  volume={3},
  number={1998},
  pages={1--10},
  year={1998}
}

@article{elekesvarious,
  title={On the Various Semantics of Similarity in Word Embedding Models},
  author={Elekes, {\'A}bel and Sch{\"a}ler, Martin and B{\"o}hm, Klemens},
  journal={Karlsruhe Reports in Informatics},
  year={2017},
  number={3},
  volum={3},
  issn={2190-4782},
  publisher={ Karlsruhe Institute of Technology}
}

@article{wilks1998grammar,
  title={The grammar of sense: Using part-of-speech tags as a first step in semantic disambiguation},
  author={Wilks, Yorick and Stevenson, Mark},
  journal={Natural Language Engineering},
  volume={4},
  number={02},
  pages={135--143},
  year={1998},
  publisher={Cambridge Univ Press}
}

@article{pang2008opinion,
  title={Opinion mining and sentiment analysis},
  author={Pang, Bo and Lee, Lillian and others},
  journal={Foundations and Trends{\textregistered} in Information Retrieval},
  volume={2},
  number={1--2},
  pages={1--135},
  year={2008},
  publisher={Now Publishers, Inc.}
}

@article{stevenson2003word,
  title={Word sense disambiguation},
  author={Stevenson, Mark and Wilks, Yorick},
  journal={The Oxford Handbook of Comp. Linguistics},
  pages={249--265},
  year={2003}
}

@inproceedings{metsis2006spam,
  title={Spam filtering with naive bayes-which naive bayes?},
  author={Metsis, Vangelis and Androutsopoulos, Ion and Paliouras, Georgios},
  booktitle={CEAS},
  volume={17},
  pages={28--69},
  year={2006}
}

@inproceedings{pang2002thumbs,
  title={Thumbs up?: sentiment classification using machine learning techniques},
  author={Pang, Bo and Lee, Lillian and Vaithyanathan, Shivakumar},
  booktitle={Proceedings of the ACL-02 conference on Empirical methods in natural language processing-Volume 10},
  pages={79--86},
  year={2002},
  organization={Association for Computational Linguistics}
}

@inproceedings{leuski2001evaluating,
  title={Evaluating document clustering for interactive information retrieval},
  author={Leuski, Anton},
  booktitle={Proceedings of the tenth international conference on Information and knowledge management},
  pages={33--40},
  year={2001},
  organization={ACM}
}

@article{forman2003extensive,
  title={An extensive empirical study of feature selection metrics for text classification},
  author={Forman, George},
  journal={Journal of machine learning research},
  volume={3},
  number={Mar},
  pages={1289--1305},
  year={2003}
}

@article{CHEN20095432,
title = "Feature selection for text classification with Naïve Bayes",
journal = "Expert Systems with Applications",
volume = "36",
number = "3",
pages = "5432 - 5435",
year = "2009",
note = "",
issn = "0957-4174",
doi = "http://dx.doi.org/10.1016/j.eswa.2008.06.054",
url = "http://www.sciencedirect.com/science/article/pii/S0957417408003564",
author = "Jingnian Chen and Houkuan Huang and Shengfeng Tian and Youli Qu",
keywords = "Text classification",
}

@article{dai2004comparative,
  title={A comparative study on feature selection in Chinese text categorization},
  author={Dai, Liuling and Huang, Heyan and Chen, Zhaoxiong},
  volume={18},
  number={1},
  pages={26--32},
  year={2004}
}

@article{MLADENIC200345,
title = "Feature selection on hierarchy of web documents",
journal = "Decision Support Systems",
volume = "35",
number = "1",
pages = "45 - 87",
year = "2003",
note = "Web Retrieval and Mining",
issn = "0167-9236",
doi = "http://dx.doi.org/10.1016/S0167-9236(02)00097-0",
url = "http://www.sciencedirect.com/science/article/pii/S0167923602000970",
author = "Dunja Mladenić and Marko Grobelnik",
}

@article{ikonomakis2005text,
  title={Text classification using machine learning techniques.},
  author={Ikonomakis, M and Kotsiantis, S and Tampakas, V},
  journal={WSEAS transactions on computers},
  volume={4},
  number={8},
  pages={966--974},
  year={2005}
}

@inproceedings{li2009framework,
  title={A framework of feature selection methods for text categorization},
  author={Li, Shoushan and Xia, Rui and Zong, Chengqing and Huang, Chu-Ren},
  booktitle={Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP: Volume 2-Volume 2},
  pages={692--700},
  year={2009},
  organization={Association for Computational Linguistics}
}

@article{harris1954distributional,
  title={Distributional structure},
  author={Harris, Zellig S},
  journal={Word},
  volume={10},
  number={2-3},
  pages={146--162},
  year={1954},
  publisher={Taylor \& Francis}
}

@article{landauer1998introduction,
  title={An introduction to latent semantic analysis},
  author={Landauer, Thomas K and Foltz, Peter W and Laham, Darrell},
  journal={Discourse processes},
  volume={25},
  number={2-3},
  pages={259--284},
  year={1998},
  publisher={Taylor \& Francis}
}

@article{golub1970singular,
  title={Singular value decomposition and least squares solutions},
  author={Golub, Gene H and Reinsch, Christian},
  journal={Numerische mathematik},
  volume={14},
  number={5},
  pages={403--420},
  year={1970},
  publisher={Springer}
}

@inproceedings{Batra2010,
abstract = {Latent Semantic Indexing (LSI), a well known technique in Information Retrieval has been partially successful in text retrieval and no major breakthrough has been achieved in text classification as yet. A significant step forward in this regard was made by Hofmann[3], who presented the probabilistic LSI (PLSI) model, as an alternative to LSI. If we wish to consider exchangeable representations for documents and words, PLSI is not successful which further led to the Latent Dirichlet Allocation (LDA) model [4]. A new local Latent Semantic Indexing method has been proposed by some authors called "Local Relevancy Ladder-Weighted LSI" (LRLW-LSI) to improve text classification [5]. In this paper we study LSI and its variants in detail, analyze the role played by them in text classification and conclude with future directions in this area. {\textcopyright} Springer Science+Business Media B.V. 2010.},
author = {Batra, Shalini and Bawa, Seema},
booktitle = {Advanced Techniques in Computing Sciences and Software Engineering},
doi = {10.1007/978-90-481-3660-5-53},
isbn = {9789048136599},
keywords = {LDA,LSI,PLSI,Text classification,VLSI},
pages = {313--316},
title = {{Using LSI and its variants in text classification}},
year = {2010}
}

@article{Hofmann1999,
abstract = {Probabilistic Laten t Seman tic Indexing is a no el approac v h to automated documen t indexing whic h is based on a sta? tistical laten t class model for factor analysis of coun t data? Fitted from a training corpus of text documen ts b y a gen? eralization of the Expectation Maximization algorithm? the utilized model is able to deal with domain?speci?c synon ym y as w ell as with polysemous w ords? In con trast to standard Laten t Seman tic Indexing ?LSI? b y Singular V alue Decom? position? the probabilistic v arian t has a solid statistical foun? dation and de?nes a proper generativ e data model? Retriev al experimen ts on a n um ber of test collections indicate sub? stan tial performance gains o er direct term matc v hing meth? odsasw ell as o er LSI? In particular? the com v bination of models with di?eren t dimensionalities has pro en to be ad? v v an tageous? ?},
archivePrefix = {arXiv},
arxivId = {2073829},
author = {Hofmann, Thomas},
doi = {10.1021/ac801303x},
eprint = {2073829},
isbn = {1581130961},
issn = {00032700},
journal = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval},
pages = {50--57},
pmid = {18989936},
title = {{Probabilistic latent semantic indexing}},
year = {1999}
}


@inproceedings{Ding2008,
abstract = {The task of Text Classification (TC) is to automatically assign natural language texts with thematic categories from a predefined category set. And Latent Semantic Indexing (LSI) is a well known technique in Information Retrieval, especially in dealing with polysemy (one word can have different meanings) and synonymy (different words are used to describe the same concept), but it is not an optimal representation for text classification. It always drops the text classification performance when being applied to the whole training set (global LSI) because this completely unsupervised method ignores class discrimination while only concentrating on representation. Some local LSI methods have been proposed to improve the classification by utilizing class discrimination information. However, their performance improvements over original term vectors are still very limited. In this paper, we propose a new local Latent Semantic Indexing method called "Local Relevancy Ladder-Weighted LSI" to improve text classification. And separate matrix singular value decomposition (SVD) was used to reduce the dimension of the vector space on the transformed local region of each class. Experimental results show that our method is much better than global LSI and traditional local LSI methods on classification within a much smaller LSI dimension. 2008 Springer-Verlag Berlin Heidelberg.},
author = {Ding, Wang and Yu, Songnian and Yu, Shanqing and Wei, Wei and Wang, Qianfeng},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
doi = {10.1007/978-3-540-79721-0_66},
isbn = {3540797203},
issn = {03029743},
keywords = {Data mining,Latent Semantic Indexing (LSI),Text classification},
pages = {483--490},
title = {{LRLW-LSI: An improved Latent Semantic Indexing (LSI) text classifier}},
volume = {5009 LNAI},
year = {2008}
}

@inproceedings{gong2001generic,
  title={Generic text summarization using relevance measure and latent semantic analysis},
  author={Gong, Yihong and Liu, Xin},
  booktitle={Proceedings of the 24th annual international ACM SIGIR conference on Research and development in information retrieval},
  pages={19--25},
  year={2001},
  organization={ACM}
}

@article{Bradford2005,
abstract = {Intelligence analysts are often faced with large data collections within which information relevant to their interests may be very sparse. Existing mechanisms for searching such data collections present difficulties even when the specific nature of the information being sought is known. Finding unknown information using these mechanisms is very inefficient. This paper presents an approach to this problem, based on iterative application of the technique of latent semantic indexing. In this approach, the body of existing knowledge on the analytic topic of interest is itself used as a query in discovering new relevant information. Performance of the approach is demonstrated on a collection of one million documents. The approach is shown to be highly efficient at discovering new information.},
author = {Bradford, R B},
doi = {10.1007/11427995_31},
isbn = {3-540-25999-6},
issn = {03029743},
journal = {Intelligence and Security Informatics},
pages = {374--380},
title = {{Efficient Discovery of New Information in Large Text Databases}},
url = {http://dx.doi.org/10.1007/11427995{\_}31},
year = {2005}
}

@inproceedings{gee2003using,
  title={Using latent semantic indexing to filter spam},
  author={Gee, Kevin R},
  booktitle={Proceedings of the 2003 ACM symposium on Applied computing},
  pages={460--464},
  year={2003},
  organization={ACM}
}

@inproceedings{pino2009application,
  title={An application of latent semantic analysis to word sense discrimination for words with related and unrelated meanings},
  author={Pino, Juan and Eskenazi, Maxine},
  booktitle={Proceedings of the Fourth Workshop on Innovative Use of NLP for Building Educational Applications},
  pages={43--46},
  year={2009},
  organization={Association for Computational Linguistics}
}

@article{Zhang2011,
abstract = {One of the main themes in text mining is text representation, which is fundamental and indispensable for text-based intellegent information processing. Generally, text representation inludes two tasks: indexing and weighting. This paper has comparatively studied TFIDF, LSI and multi-word for text representation. We used a Chinese and an English document collection to respectively evaluate the three methods in information retreival and text categorization. Experimental results have demonstrated that in text categorization, LSI has better performance than other methods in both document collections. Also, LSI has produced the best performance in retrieving English documents. This outcome has shown that LSI has both favorable semantic and statistical quality and is different with the claim that LSI can not produce discriminative power for indexing. {\textcopyright} 2010 Elsevier Ltd. All rights reserved.},
author = {Zhang, Wen and Yoshida, Taketoshi and Tang, Xijin},
doi = {10.1016/j.eswa.2010.08.066},
isbn = {09574174},
issn = {09574174},
journal = {Expert Systems with Applications},
keywords = {Information retrieval,LSI,Multi-word,TFIDF,Text categorization,Text classification,Text representation},
number = {3},
pages = {2758--2765},
title = {{A comparative study of TF*IDF, LSI and multi-words for text classification}},
volume = {38},
year = {2011}
}

@article{Zelikovitz2001,
abstract = {We present work in progress that uses Latent Semantic Indexing (LSI) in conjunction with background knowledge and unlabeled examples to improve text classification accuracy. The singular value decomposition (SVD) that is performed by LSI is done on an expanded term by document matrix that includes the labeled training examples as well as the unlabeled examples. We report classification accuracy on different data sets bothwith and without the inclusionof backgroundknowledge and compare it to other known work.},
author = {Zelikovitz, Sarah and Hirsh, Haym},
journal = {Learning},
title = {{Improving Text Classification with LSI Using Background Knowledge}},
url = {http://scholar.google.com/scholar?q=intitle:Improving+Text+Classification+with+LSI+Using+Background+Knowledge{\#}0},
year = {2001}
}


@article{Blei2003,
abstract = {We describe latent Dirichlet allocation (LDA), a generative probabilistic model for collections of discrete data such as text corpora. LDA is a three-level hierarchical Bayesian model, in which each item of a collection is modeled as a finite mixture over an underlying set of topics. Each topic is, in turn, modeled as an infinite mixture over an underlying set of topic probabilities. In the context of text modeling, the topic probabilities provide an explicit representation of a document. We present efficient approximate inference techniques based on variational methods and an EM algorithm for empirical Bayes parameter estimation. We report results in document modeling, text classification, and collaborative filtering, comparing to a mixture of unigrams model and the probabilistic LSI model.},
archivePrefix = {arXiv},
arxivId = {1111.6189v1},
author = {Blei, David M and Edu, Blei@cs Berkeley and Ng, Andrew Y and Edu, Ang@cs Stanford and Jordan, Michael I and Edu, Jordan@cs Berkeley},
doi = {10.1162/jmlr.2003.3.4-5.993},
eprint = {1111.6189v1},
isbn = {9781577352815},
issn = {15324435},
journal = {Journal of Machine Learning Research},
pages = {993--1022},
pmid = {21362469},
title = {{Latent Dirichlet Allocation}},
volume = {3},
year = {2003}
}

@article{hu2009latent,
  title={Latent dirichlet allocation for text, images, and music},
  author={Hu, Diane J},
  journal={University of California, San Diego. Retrieved April},
  volume={26},
  pages={2013},
  year={2009}
}

@article{Blei2010,
abstract = {We introduce supervised latent Dirichlet allocation (sLDA), a statistical model of labelled documents. The model accommodates a variety of response types. We derive an approximate maximum-likelihood procedure for parameter estimation, which relies on variational methods to handle intractable posterior expectations. Prediction problems motivate this research: we use the fitted model to predict response values for new documents. We test sLDA on two real-world problems: movie ratings predicted from reviews, and the political tone of amendments in the U.S. Senate based on the amendment text. We illustrate the benefits of sLDA versus modern regularized regression, as well as versus an unsupervised LDA analysis followed by a separate regression.},
archivePrefix = {arXiv},
arxivId = {1003.0783},
author = {Blei, David M. and McAuliffe, Jon D.},
doi = {10.1002/asmb.540},
eprint = {1003.0783},
file = {:C$\backslash$:/Users/thesp/home/uni/masterarbeit/literatur/3328-supervised-topic-models.pdf:pdf},
isbn = {160560352X},
issn = {15241904},
pages = {1--8},
title = {{Supervised Topic Models}},
url = {http://arxiv.org/abs/1003.0783},
year = {2010}
}


@article{Zhu2012,
abstract = {A supervised topic model can use side information such as ratings or labels associated with doc- uments or images to discover more predictive low dimensional topical representations of the data. However, existing supervised topic models predominantly employ likelihood-driven objective func- tions for learning and inference, leaving the popular and potentially powerful max-margin principle unexploited for seeking predictive representations of data and more discriminative topic bases for the corpus. In this paper, we propose the maximum entropy discrimination latent Dirichlet alloca- tion (MedLDA) model, which integrates the mechanism behind the max-margin prediction models (e.g., SVMs) with the mechanism behind the hierarchical Bayesian topic models (e.g., LDA) un- der a unified constrained optimization framework, and yields latent topical representations that are more discriminative and more suitable for prediction tasks such as document classification or re- gression. The principle underlying the MedLDA formalism is quite general and can be applied for jointly max-margin and maximum likelihood learning of directed or undirected topic models when supervising side information is available. Efficient variational methods for posterior inference and parameter estimation are derived and extensive empirical studies on several real data sets are also provided. Our experimental results demonstrate qualitatively and quantitatively that MedLDA could: 1) discover sparse and highly discriminative topical representations; 2) achieve state of the art prediction performance; and 3) be more efficient than existing supervised topic models, espe- cially for classification.},
archivePrefix = {arXiv},
arxivId = {0912.5507},
author = {Zhu, Jun and Ahmed, a and Xing, Ep},
doi = {10.1145/1553374.1553535},
eprint = {0912.5507},
isbn = {978-1-60558-516-1},
issn = {1532-4435},
journal = {Journal of Machine Learning Research},
keywords = {latent dirichlet allocation,max-margin learning,maximum entropy discrimination,supervised topic models,support vector machines},
pages = {2237--2278},
title = {{MedLDA: Maximum Margin Supervised Topic Models}},
url = {http://jmlr.csail.mit.edu/papers/volume13/zhu12a/zhu12a.pdf},
volume = {13},
year = {2012}
}

@inproceedings{Chen2011,
abstract = {Understanding the rapidly growing short text is very important. Short text is different from tra- ditional documents in its shortness and sparsity, which hinders the application of conventional machine learning and text mining algorithms. Two major approaches have been exploited to enrich the representation of short text. One is to fetch contextual information of a short text to directly add more text; the other is to derive latent topics from existing large corpus, which are used as features to enrich the representation of short text. The latter approach is elegant and efficient in most cases. The major trend along this direction is to derive latent top- ics of certain granularity through well-known topic models such as latent Dirichlet allocation (LDA). However, topics of certain granularity are usually not sufficient to set up effective feature spaces. In this paper, wemove forward along this direction by proposing an method to leverage topics at multiple granularity, which can model the short text more precisely. Taking short text classification as an ex- ample, we compared our proposed method with the state-of-the-art baseline over one open data set. Our method reduced the classification error by 20.25{\%} and 16.68{\%}respectively on two classifiers},
author = {Chen, Mengen and Jin, Xiaoming and Shen, Dou},
booktitle = {IJCAI International Joint Conference on Artificial Intelligence},
doi = {10.5591/978-1-57735-516-8/IJCAI11-298},
isbn = {9781577355120},
issn = {10450823},
pages = {1776--1781},
title = {{Short text classification improved by learning multi-granularity topics}},
year = {2011}
}

@inproceedings{phan2008learning,
  title={Learning to classify short and sparse text \& web with hidden topics from large-scale data collections},
  author={Phan, Xuan-Hieu and Nguyen, Le-Minh and Horiguchi, Susumu},
  booktitle={Proceedings of the 17th international conference on World Wide Web},
  pages={91--100},
  year={2008},
  organization={ACM}
}

@inproceedings{Girolami2003,
abstract = {Latent Dirichlet Allocation (LDA) is a fully generative approach to language modelling which overcomes the inconsistent generative semantics of Probabilistic Latent Semantic Indexing (PLSI). This paper shows that PLSI is a maximum a posteriori estimated LDA model under a uniform Dirichlet prior, therefore the perceived shortcomings of PLSI can be resolved and elucidated within the LDA framework.},
author = {Girolami, Mark and Kab{\'{a}}n, Ata},
booktitle = {Proceedings of the 26th annual international ACM SIGIR conference on Research and development in informaion retrieval  - SIGIR '03},
doi = {10.1145/860435.860537},
isbn = {1581136463},
issn = {1581136463},
pages = {433},
title = {{On an equivalence between PLSI and LDA}},
url = {http://portal.acm.org/citation.cfm?doid=860435.860537},
year = {2003}
}

@article{mikolov2013efficient,
  title={Efficient estimation of word representations in vector space},
  author={Mikolov, Tomas and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
  journal={arXiv preprint arXiv:1301.3781},
  year={2013}
}

@inproceedings{Pennington2014,
abstract = {Recent methods for learning vector space representations of words have succeeded in capturing fine-grained semantic and syntactic regularities using vector arith- metic, but the origin of these regularities has remained opaque. We analyze and make explicit the model properties needed for such regularities to emerge in word vectors. The result is a new global log- bilinear regression model that combines the advantages of the two major model families in the literature: global matrix factorization and local context window methods. Our model efficiently leverages statistical information by training only on the nonzero elements in a word-word co- occurrence matrix, rather than on the en- tire sparse matrix or on individual context windows in a large corpus. On a recent word analogy task our model obtains 75{\%} accuracy, an improvement of 11{\%} over Mikolov et al. (2013). It also outperforms related word vector models on similarity tasks and named entity recognition.},
archivePrefix = {arXiv},
arxivId = {1504.06654},
author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
booktitle = {Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
doi = {10.3115/v1/D14-1162},
eprint = {1504.06654},
isbn = {9781937284961},
issn = {10495258},
pages = {1532--1543},
pmid = {1710995},
title = {{Glove: Global Vectors for Word Representation}},
url = {http://aclweb.org/anthology/D14-1162},
year = {2014}
}

@article{Levy2015,
abstract = {Recent trends suggest that neural-network-inspired word embedding models outperform traditional count-based distri-butional models on word similarity and analogy detection tasks. We reveal that much of the performance gains of word embeddings are due to certain system design choices and hyperparameter op-timizations, rather than the embedding algorithms themselves. Furthermore, we show that these modifications can be transferred to traditional distributional models, yielding similar gains. In contrast to prior reports, we observe mostly local or insignificant performance differences between the methods, with no global advantage to any single approach over the others.},
archivePrefix = {arXiv},
arxivId = {1103.0398},
author = {Levy, Omer and Goldberg, Yoav and Dagan, Ido},
doi = {10.1186/1472-6947-15-S2-S2},
eprint = {1103.0398},
file = {:C$\backslash$:/Users/thesp/home/uni/masterarbeit/literatur/570-1656-1-PB.pdf:pdf},
isbn = {1472-6947 (Electronic)$\backslash$r1472-6947 (Linking)},
issn = {2307-387X},
journal = {Transactions of the Association for Computational Linguistics},
pages = {211--225},
pmid = {26099735},
title = {{Improving Distributional Similarity with Lessons Learned from Word Embeddings}},
url = {https://tacl2013.cs.columbia.edu/ojs/index.php/tacl/article/view/570},
volume = {3},
year = {2015}
}

@inproceedings{JosephLilleberge2015,
abstract = {With the rapid expansion of new available information presented to us online on a daily basis, text classification becomes imperative in order to classify and maintain it. Word2vec offers a unique perspective to the text mining community. By converting words and phrases into a vector representation, word2vec takes an entirely new approach on text classification. Based on the assumption that word2vec brings extra semantic features that helps in text classification, our work demonstrates the effectiveness of word2vec by showing that tf-idf and word2vec combined can outperform tf-idf because word2vec provides complementary features (e.g. semantics that tf-idf can't capture) to tf-idf. Our results show that the combination of word2vec weighted by tf-idf and tf-idf does not outperform tf-idf consistently. It is consistent enough to say the combination of the two can outperform either individually.},
author = {{Joseph, Lilleberge}, Computer Science Department SouthWest Minnesota State University and {Yun, Zhu}, Computer Science Department Georgia State University and {Yanqing, Zhang}, Computer Science Department Georgia State University},
booktitle = {20151IEE 14th Inl'l Coni. on Cognitive Inlormatics {\&} Cognitive Computing IIccrcn51},
doi = {10.1109/ICCI-CC.2015.7259377},
isbn = {9181461312909},
pages = {4--8},
title = {{Support Vector Machines and Word2vec for Text Classification with Semantic Features}},
year = {2015}
}

@article{Lai2015,
abstract = {Text classification is a foundational task in many NLP applications. Traditional text classifiers often rely on many human-designed features, such as dictionaries, knowledge bases and special tree kernels. In contrast to traditional methods, we introduce a recurrent con-volutional neural network for text classification with-out human-designed features. In our model, we apply a recurrent structure to capture contextual information as far as possible when learning word representations, which may introduce considerably less noise compared to traditional window-based neural networks. We also employ a max-pooling layer that automatically judges which words play key roles in text classification to cap-ture the key components in texts. We conduct experi-ments on four commonly used datasets. The experimen-tal results show that the proposed method outperforms the state-of-the-art methods on several datasets, partic-ularly on document-level datasets.},
author = {Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
isbn = {9781577357018},
journal = {Twenty-Ninth AAAI Conference on Artificial Intelligence},
keywords = {NLP and Machine Learning Track},
pages = {2267--2273},
title = {{Recurrent Convolutional Neural Networks for Text Classification}},
year = {2015}
}

@article{Levy2014,
abstract = {While continuous word embeddings are gaining popularity, current models are based solely on linear contexts. In this work, we generalize the skip-gram model with negative sampling introduced by Mikolov et al. to include arbitrary con-texts. In particular, we perform exper-iments with dependency-based contexts, and show that they produce markedly different embeddings. The dependency-based embeddings are less topical and ex-hibit more functional similarity than the original skip-gram embeddings.},
author = {Levy, Omer and Goldberg, Yoav},
doi = {10.3115/v1/P14-2050},
file = {:C$\backslash$:/Users/thesp/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Levy, Goldberg - 2014 - Dependencybased word embeddings.pdf:pdf},
isbn = {9781937284732},
journal = {Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics},
mendeley-groups = {Master Thesis/Word Embeddings},
pages = {302--308},
pmid = {1627600},
title = {{Dependencybased word embeddings}},
volume = {2},
year = {2014}
}

@article{Rothenhausler2009,
abstract = {We present the results of clustering exper- iments with a number of different evalu- ation sets using dependency based word spaces. Contrary to previous results we found a clear advantage using a parsed corpus over word spaces constructed with the help of simple patterns. We achieve considerable gains in performance over these spaces ranging between 9 and 13{\%} in absolute terms of cluster purity.},
author = {Rothenh{\"{a}}usler, Klaus and Sch{\"{u}}tze, Hinrich},
journal = {In Proceedings of the EACL 2009 Workshop on GEMS: GEometical Models of Natural Language Semantics, Athens, Greece},
keywords = {Distributional semantics},
number = {March},
pages = {17--24},
title = {{Unsupervised Classification with Dependency Based Word Spaces}},
year = {2009}
}

@article{Komninos2016,
abstract = {We compare different word embeddings from a standard window based skipgram model, a skipgram model trained using dependency context features and a novel skipgram variant that utilizes additional information from dependency graphs. We explore the effectiveness of the different types of word embeddings for word similarity and sentence classification tasks. We consider three common sentence classification tasks: question type classification on the TREC dataset, binary sentiment classification on Stanford's Sentiment Treebank and semantic relation classification on the SemEval 2010 dataset. For each task we use three different classification methods: a Support Vector Machine, a Convolutional Neural Network and a Long Short Term Memory Network. Our experiments show that dependency based embeddings outperform standard window based embeddings in most of the settings, while using dependency context embeddings as additional features improves performance in all tasks regardless of the classification method. Our embeddings and code are available at},
author = {Komninos, Alexandros},
isbn = {9781941643914},
journal = {Naacl2016},
pages = {1490--1500},
title = {{Dependency Based Embeddings for Sentence Classification Tasks}},
url = {https://www.cs.york.ac.uk/nlp/},
year = {2016}
}

@article{Lewis1998,
abstract = {. The naive Bayes classifier, currently experiencing a renaissance in machine learning, has long been a core technique in information retrieval. We review some of the variations of naive Bayes models used for text retrieval and classification, focusing on the distributional assumptions made about word occurrences in documents. 1 Introduction The naive Bayes classifier, long a favorite punching bag of new classification techniques, has recently emerged as a focus of research itself in machine...},
author = {Lewis, David D. and N$\backslash$'{\{}e{\}}dellec, Claire and Rouveirol, C$\backslash$'{\{}e{\}}line},
doi = {10.1007/BFb0026666},
isbn = {3-540-64417-2},
issn = {0302-9743 (Print) 1611-3349 (Online)},
journal = {Machine Learning: ECML-98},
pages = {4----15},
title = {{Naive (Bayes) at Forty: The Independence Assumption in Information Retrieval}},
url = {http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.11.8264{\%}5Cnhttp://cat.inist.fr/?aModele=afficheN{\&}cpsidt=2042276{\%}5Cnhttp://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.33.8397},
year = {1998}
}


@article{Eyheramendy2003,
abstract = {This paper empirically compares the performance of four probabilistic models for text classification- Poisson, Bernoulli, Multinomial and Negative Binomial. We examine the "naive Bayes" assumption in the four models and show that the multinomial model is a modified naive Bayes Poisson model that assumes independence of document length and document class. Despite the fact that this last assumption might not be correct in many situations, we find that, in general, relaxing it does not change the performance of the classifier.},
author = {Eyheramendy, Susana and Lewis, David and Madigan, David},
journal = {Artificial Intelligence and Statistics},
title = {{On the naive bayes model for text categorization}},
url = {http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.20.4949},
year = {2003}
}


@article{kim2006some,
  title={Some effective techniques for naive bayes text classification},
  author={Kim, Sang-Bum and Han, Kyoung-Soo and Rim, Hae-Chang and Myaeng, Sung Hyon},
  journal={IEEE transactions on knowledge and data engineering},
  volume={18},
  number={11},
  pages={1457--1466},
  year={2006},
  publisher={IEEE}
}

@article{Mendoza2012,
abstract = {Purpose -- Automatic text categorization has applications in several domains, for example e-mail spam detection, sexual content filtering, directory maintenance, and focused crawling, among others. Most information retrieval systems contain several components which use text categorization methods. One of the first text categorization methods was designed using a naive Bayes representation of the text. Currently, a number of variations of naive Bayes have been discussed. The purpose of this paper is to evaluate naive Bayes approaches on text categorization introducing new competitive extensions to previous approaches. Design/methodology/approach -- The paper focuses on introducing a new Bayesian text categorization method based on an extension of the naive Bayes approach. Some modifications to document representations are introduced based on the well-known BM25 text information retrieval method. The performance of the method is compared to several extensions of naive Bayes using benchmark datasets designed for this purpose. The method is compared also to training-based methods such as support vector machines and logistic regression. Findings -- The proposed text categorizer outperforms state-of-the-art methods without introducing new computational costs. It also achieves performance results very similar to more complex methods based on criterion function optimization as support vector machines or logistic regression. Practical implications -- The proposed method scales well regarding the size of the collection involved. The presented results demonstrate the efficiency and effectiveness of the approach. Originality/value -- The paper introduces a novel naive Bayes text categorization approach based on the well-known BM25 information retrieval model, which offers a set of good properties for this problem. Adapted from the source document.},
author = {Mendoza, Marcelo},
doi = {10.1108/17440081211222591},
isbn = {1352760081089},
issn = {1744-0084},
journal = {International Journal of Web Information Systems},
number = {1},
pages = {55--72},
title = {{A new term‐weighting scheme for na{\"{i}}ve Bayes text categorization}},
url = {http://www.emeraldinsight.com/doi/10.1108/17440081211222591},
volume = {8},
year = {2012}
}


@article{Rennie2003,
abstract = {Naive Bayes is often used as a baseline in text classification because it is fast and easy to implement. Its severe assumptions make such eciency possible but also adversely affect the quality of its results. In this paper we propose simple, heuristic solutions to some of the problems with Naive Bayes classifiers, addressing both systemic issues as well as problems that arise because text is not actually generated according to a multinomial model. We find that our simple corrections result in a fast algorithm that is competitive with stateof-the-art text classification algorithms such as the Support Vector Machine.},
author = {Rennie, Jason D M and Shih, Lawrence and Teevan, Jaime and Karger, David R},
doi = {10.1186/1477-3155-8-16},
isbn = {978-1-57735-189-4},
issn = {14773155},
journal = {Proceedings of the Twentieth International Conference on Machine Learning (ICML)-2003)},
number = {1973},
pages = {616--623},
pmid = {20630072},
title = {{Tackling the Poor Assumptions of Naive Bayes Text Classifiers}},
url = {http://www.aaai.org/Papers/ICML/2003/ICML03-081.pdf},
volume = {20},
year = {2003}
}

@inproceedings{dumais1998inductive,
  title={Inductive learning algorithms and representations for text categorization},
  author={Dumais, Susan and Platt, John and Heckerman, David and Sahami, Mehran},
  booktitle={Proceedings of the seventh international conference on Information and knowledge management},
  pages={148--155},
  year={1998},
  organization={ACM}
}

@book{vapnik2013nature,
  title={The nature of statistical learning theory},
  author={Vapnik, Vladimir},
  year={2013},
  publisher={Springer science \& business media}
}

@inproceedings{Yang1999,
abstract = {This paper reports a controlled study with statistical signifi cance tests on five text categorization methods: the Support Vector Machines (SVM), a kNearest Neighbor (kNN) clas sifier, a neural network (NNet) approach, the Linear Least squares Fit (LLSF) mapping and a Naive Bayes (NB) classi fier. We focus on the robustness of these methods in dealing with a skewed category distribution, and their performance as function of the trainingset category frequency. Our re sults show that SVM, kNN and LLSF significantly outper form NNet and NB when the number of positive training instances per category are small (less than ten), and that all the methods perform comparably when the categories are sufficiently common (over 300 instances).},
author = {Yang, Yiming and Liu, Xin},
booktitle = {Proceedings of the 22nd annual international ACM SIGIR conference on Research and development in information retrieval  - SIGIR '99},
doi = {10.1145/312624.312647},
isbn = {1581130961},
issn = {1581130961},
pages = {42--49},
pmid = {15068922},
title = {{A re-examination of text categorization methods}},
url = {http://portal.acm.org/citation.cfm?doid=312624.312647},
year = {1999}
}

@article{Hornik1989,
abstract = {This paper rigorously establishes that standard multilayer feedforward networks with as few as one hidden layer using arbitrary squashing functions are capable of approximating any Borel measurable function from one finite dimensional space to another to any desired degree of accuracy, provided sufficiently many hidden units are available. In this sense, multilayer feedforward networks are a class of universal approximators. ?? 1989.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Hornik, Kurt and Stinchcombe, Maxwell and White, Halbert},
doi = {10.1016/0893-6080(89)90020-8},
eprint = {arXiv:1011.1669v3},
isbn = {08936080 (ISSN)},
issn = {08936080},
journal = {Neural Networks},
keywords = {Back-propagation networks,Feedforward networks,Mapping networks,Network representation capability,Sigma-Pi networks,Squashing functions,Stone-Weierstrass Theorem,Universal approximation},
number = {5},
pages = {359--366},
pmid = {74},
title = {{Multilayer feedforward networks are universal approximators}},
volume = {2},
year = {1989}
}

@article{zhang2006multilabel,
  title={Multilabel neural networks with applications to functional genomics and text categorization},
  author={Zhang, Min-Ling and Zhou, Zhi-Hua},
  journal={IEEE transactions on Knowledge and Data Engineering},
  volume={18},
  number={10},
  pages={1338--1351},
  year={2006},
  publisher={IEEE}
}

@inproceedings{zhang2015character,
  title={Character-level convolutional networks for text classification},
  author={Zhang, Xiang and Zhao, Junbo and LeCun, Yann},
  booktitle={Advances in neural information processing systems},
  pages={649--657},
  year={2015}
}

@inproceedings{lai2015recurrent,
  title={Recurrent Convolutional Neural Networks for Text Classification.},
  author={Lai, Siwei and Xu, Liheng and Liu, Kang and Zhao, Jun},
  booktitle={AAAI},
  volume={333},
  pages={2267--2273},
  year={2015}
}

@article{kim2014convolutional,
  title={Convolutional neural networks for sentence classification},
  author={Kim, Yoon},
  journal={arXiv preprint arXiv:1408.5882},
  year={2014}
}

@article{Joachims1998,
abstract = {This paper explores the use of Support Vector Machines (SVMs) for learning text classifiers from examples. It analyzes the particular properties of learning with text data and identifes why SVMs are appropriate for this task. Empirical results support the theoretical findings. SVMs achieve substantial improvements over the currently best performing methods and behave robustly over a variety of different learning tasks. Furthermore, they are fully automatic, eliminating the need for manual parameter tuning.},
author = {Joachims, Thorsten},
doi = {10.1007/BFb0026683},
isbn = {3-540-64417-2},
issn = {0945-1129},
journal = {Proceedings of the 10th European Conference on Machine Learning ECML '98},
pages = {137--142},
pmid = {9934216},
title = {{Text Categorization with Suport Vector Machines: Learning with Many Relevant Features}},
year = {1998}
}

@inproceedings{lu2015deep,
  title={Deep Multilingual Correlation for Improved Word Embeddings.},
  author={Lu, Ang and Wang, Weiran and Bansal, Mohit and Gimpel, Kevin and Livescu, Karen},
  booktitle={HLT-NAACL},
  pages={250--256},
  year={2015}
}

@article{casellar,
  title={R. 2001, Statistical Inference},
  author={Casella, George and Berger, Roger},
  journal={Duxbury Press},
  year={2001}
}

@article{myung2003tutorial,
  title={Tutorial on maximum likelihood estimation},
  author={Myung, In Jae},
  journal={Journal of mathematical Psychology},
  volume={47},
  number={1},
  pages={90--100},
  year={2003},
  publisher={Elsevier}
}

@book{zacks1971theory,
  title={The theory of statistical inference},
  author={Zacks, Shelemyahu},
  volume={34},
  year={1971},
  publisher={Wiley New York}
}

@article{agresti1998approximate,
  title={Approximate is better than “exact” for interval estimation of binomial proportions},
  author={Agresti, Alan and Coull, Brent A},
  journal={The American Statistician},
  volume={52},
  number={2},
  pages={119--126},
  year={1998},
  publisher={Taylor \& Francis}
}

@inproceedings{liu2015learning,
  title={Learning Context-Sensitive Word Embeddings with Neural Tensor Skip-Gram Model.},
  author={Liu, Pengfei and Qiu, Xipeng and Huang, Xuanjing},
  booktitle={IJCAI},
  pages={1284--1290},
  year={2015}
}

@inproceedings{komninos2016dependency,
  title={Dependency Based Embeddings for Sentence Classification Tasks.},
  author={Komninos, Alexandros and Manandhar, Suresh},
  booktitle={HLT-NAACL},
  pages={1490--1500},
  year={2016}
}

@inproceedings{saif2012alleviating,
  title={Alleviating data sparsity for twitter sentiment analysis},
  author={Saif, Hassan and He, Yulan and Alani, Harith},
  year={2012},
  organization={CEUR Workshop Proceedings (CEUR-WS. org)}
}