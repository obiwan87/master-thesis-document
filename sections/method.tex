\chapter{Method}

In this chapter we describe a method to process a dataset prior to
classification, such that the described problems, namely the statistical
sparsity and OOV-words are reduced. Our approach consists of two main
elements:

\begin{itemize}  
  \item A dissimilarity measure for words based on both distributional and
  semantical information
  \item The usage of a state-of-the-art clutering algorithm to find groups of
  similar words

\end{itemize}


\section{Combining semantic and distributional similarity}

As shown in \ref{sec:echobot}, we can reduce the statistical sparsity of a
BoW-feature vector, by merging similar terms with semantic similarity. However,
we also showed that using only semantic similarity as a merge criterion ,
might not yield the desired improvement in classification. This is due to the
fact that semantic similar words might be distributed differently over the
classes. In this section, we will derive a novel distance measure that
integrates both semantic and distributional information. 

\subsection{Word occurences as a Bernoulli Process}
In order to derive the distributional dissimilarity between two words, it is
useful to view the word occurences as being generated by a probabilistic
process. In the classification tasks considered in this work, we assume that
there are two classes. Therefore a suitable probabilistic model would be a
\emph{bernoulli process}\textbf{[citation]}.\footnote{The 
corresponding distribution that suits a mulitclass scenario is the \emph{multinomial
distribution}.} Assume a word $v \in V$ appears $n$ times in a dataset.
We model the occurences of $v$ as random variables $Y_1, Y_2, Y_3, \ldots, Y_n$ over the
events $\Omega = \{ +, -\}$, i.e. if $v$ appears in the positive or negative
class respectively:

\begin{equation*}
	Y_i \coloneqq Y_i(\omega) = \begin{cases} 1 & \mathrm{if~} \omega = \mathrm{+}
	\\
	0 & \mathrm{if~} \omega = \mathrm{-} \end{cases}, i = 1,\ldots,n
\end{equation*}

We also introduce the random variable $X$ that indicates the total
amount of occurences in the positive class:

\begin{equation*}
	X = \sum\limits_{i=1}^n Y_i
\end{equation*}

Then the probability that for $n$ occurences of $v$, the word appears $k$ times
in the positive class is: 

\begin{equation*}
	\Pr(X=k|\theta) = \binom{n}{k} \theta^k(1-\theta)^{n-k}
\end{equation*}

The probability distribution above is also known as \emph{bernoulli or binomial
distribution}. Here $\theta \in [0,1]$ denotes the distribution parameter.
It corresponds to the probability of that a single draw of the word $v$ results in
a occurence in the positive class:

\begin{equation*}
	\Pr(Y_i = 1) = \theta
\end{equation*}

A bernoulli process $X$ with $n$ outcomes  
and parameter $\theta$ is also written as:

\begin{equation*}
	X \sim B(n, \theta)
\end{equation*}

\subsection{Merging words as a probabilistic event}

Now we address the question of the merger of two words in the context of the
introduced probabilistic bernoulli process. 
When merging two words, we combine their statistics and as a consequence the
occurences of either word collapses to one single event. 
Consider the probability two different words represented by the random
variables $X_1, X_2$ appearing $n_1, n_2$ in the dataset and $k_1, k_2$ times in
the positive class. If we assume that words occurences are indepedent, then we
have the following joint probability:

\begin{equation*}
\begin{split}
	\Pr(X_1 = k_1, X_2 = k_2; \theta_1, \theta_2) &=
	\Pr(X_1=k_1;\theta_1)\Pr(X_2=k_2;\theta_2)
	\\
	&= 	\binom{n_1}{k_1}\theta_1^{k_1}(1-\theta_1)^{n_1-k_1} \cdot
	\binom{n_2}{k_2}\theta_2^{k_2}(1-\theta_2)^{n_2-k_2}
\end{split}
\end{equation*}

When merging the words, we assume that the underlying independent events are
generated by the same bernoulli process. Formally, we have the following hypotheses: 

\begin{enumerate}
  \item $H_0$: The word observations $X_1, X_2$ are generated by two different
  processes, i.e.  generally $\theta_1 \neq \theta_2$
  \item $H_1$: The words observations $X_1, X_2$ are generated by the same
  process, i.e. $\theta_1 = \theta_2$
\end{enumerate}

For the first hypothesis, since we have no further knowledge about the
underlying parameters, we estimate $\theta_i$ to be the maximum likelihood 
estimator based on the observations: 

\begin{equation*}
	H_0: \hat{\theta}_{H_0,i} = \frac{k_i}{n_i}
\end{equation*}

For the second hypothesis, we assume that the indepedent observations $X_1, X_2$
have the same underlying distribution parameter. We express this by combining
the statistics of the observations for $X_1$ and $X_2$:

\begin{equation*}
H_1: \hat{\theta}_{H_1} =\hat{\theta}_{H_1,1} =
\hat{\theta}_{H_1,2} =
\frac{k_1 + k_2}{n_1 + n_2}
\end{equation*}
 
The situation is analogous to observing the outcomes of two different series of
coin tosses $X_1, X_2$ and considering the hypotheses that the two
observations have been generated by two different coins ($\theta_1 \neq \theta_2$) or by the same coin
($\theta_1 = \theta_2$). Given the observed data, our goal is now to calculate
the probability for each hypothesis. This can be done with using bayesian probability theory.
Moreover, since our approach operates on probabilities, the values yielded are
in the interval $[0, 1]$ and similarly scaled as the cosine distance. 

\subsection{Bayesian Hypothesis Testing}

\emph{Bayesian Hypothesis Testing} is a tool to compute the probability of
competing models based on the available observations. In our context, we want to
compare how well the occurences of two words $v_1, v_2$ in a document set $D$ is
explained before and after merging them.
We start from the proposition, that either one hypothesis or the other must be true, i.e.:

\begin{equation*}
	\Pr(H_0|D) + \Pr(H_1|D) = 1
\end{equation*}

The probabilities of the hyptheses $H_i,~i=0,1$, i.e. the words are generated by
two different bernoulli processes or by the same, can be expressed using the
bayesian rule as follows:

\begin{equation*}
\begin{split}
	\Pr(H_0|D) &= \frac{\Pr(D|H_0)\Pr(H_0)}{\Pr(H_0)\Pr(D|H_0) + \Pr(H_1)\Pr(D|H_1)} \\
			 & =\frac{1}{1+
			 \underbrace{\frac{\Pr(H_1)}{\Pr(H_0)}\frac{\Pr(D|H_1)}{\Pr(D|H_0)}}_{
			 \coloneqq B_{01}}} = (1+B_{01})^{-1} \\
    \Pr(H_1|D) &= 1 - (1 + B_{01})^{-1}= \left(1+{B_{01}}^{-1}\right)^{-1}
\end{split}
\end{equation*}

Here $B_{01}$ is called the \emph{Bayes Factor} and is often used as an
alternative to the probability to express the difference between two competing hypotheses.
Using the probability distribution function introduced  in the previous
section we obtain: 

\begin{equation*}
\begin{split}
\Pr(D|H_0) &= \Pr\left(X_1=k_1, X_2=k_2 \mid \theta_1 = \hat{\theta}_{H_0,1},
\theta_2 = \hat{\theta}_{H_0,2}\right) \\
&=
\binom{n_1}{k_1}\hat{\theta}_{H_0,1}^{k_1}\left(1-\hat{\theta}_{H_0,1}\right)^{n_1-k_1}
\cdot
	\binom{n_2}{k_2}\hat{\theta}_{H_0,2}^{k_2}\left(1-\hat{\theta}_{H_0,2}\right)^{n_2-k_2} 
\\
\Pr(D|H_1) &= \Pr\left(X_1=k_1, X_2=k_2 \mid \theta_1 = \theta_2 = \frac{k_1 +
k_2}{n_1 + n_2}\right) \\
&= \binom{n_1}{k_1}\binom{n_2}{k_2}\hat{\theta}_{H_1}^{k_1 +
k_2}\left(1-\hat{\theta}_{H_1}\right)^{n_1 + n_2 - k_1 - k_2}
\end{split}
\end{equation*}

We then insert $P(D|H_i),~i=0,1$ in the bayes factor $B_{01}$ and obtain:

\begin{equation*}
B_{01} = \frac{\Pr(H_0)}{\Pr(H_1)} \cdot
\frac{\hat{\theta}_{H_0,1}^{k_1}\left(1-\hat{\theta}_{H_0,1}\right)^{n_1-k_1}\hat{\theta}_{H_0,2}^{k_2}\left(1-\hat{\theta}_{H_0,2}\right)^{n_2-k_2}
}{\hat{\theta}_{H_1}^{k_1 + k_2}\left(1-\hat{\theta}_{H_1}\right)^{n_1 +
n_2 - k_1 - k_2}}
\end{equation*}

\subsection{Integrating external knowledge in the priors}
When there is no knowledge about the prior probabilities $\Pr(H_0), \Pr(H_1)$ a
common heuristic is to assume that every hypothesis is equally probable, i.e.
$\Pr(H_0) = \Pr(H_1) = 0.5$ \textbf{[citation]}. However, we can use the information
provided by word embeddings to approximate the priors. The calculation of word
vectors implies predicting the probability of a word appearing in a given
context. The cosine similarity between two word vectors, can be interpreted as
an approximation of the probability that two words $v_1, v_2$ in question appear
in the same context $C$ \textbf{[citation needed]}. 

\begin{equation*}
	\cos \angle (vec(v_1), vec(v_2)) \sim \Pr(\text{``}v_1, v_2~\text{appear in
	similar contexts} \text{``})
\end{equation*}

We can use this interpretation of the cosine similarity to incorporate the
knowledge captured by word embeddings in the priors. We propose to use: 

\begin{equation*}
\begin{split}
	\Pr(H_0; \alpha) &= \alpha\cdot\left(\frac 1 2 - \frac{dist(v_1,v_2)}{2}\right)
	+ \frac 1 2
	\\
	\Pr(H_1; \alpha) &= 1 - \Pr(H_0)
\end{split}
\end{equation*}

For words that are farther away in the emebedding vector space, we want to
favour the first hypothesis, i.e. that the words should not be merged. When
words are closer they tend to appear in similar contexts, which is a hint that
the two words can be merged without losing discriminative information. $\alpha$
is a linear weight that regulates how much the cosine distance affects the
priors to deviate from $0.5$. For $\alpha=0$ the cosine similarity is completely
negletcted and the hypotheses are assumed to be equally probable. For $\alpha >
0$, the priors deviate from $0.5$ proportionally to $\alpha \cdot
\frac{\mli{dist}(v_1,v_2)}{2}$.\footnote{In our formula we use
$\mli{dist}(v_1,v_2)/2$ in order to normalize the cosine distance to the interval $[0, 1]$}

Using the hypothesis probability $\Pr(H_1|D)$ we define a
distributional distance measure between two words $v, v' \in V$ as follows:

\begin{equation*}
\begin{split}
	B_{01}(\alpha) &= \frac{\Pr(H_0; \alpha)}{1-\Pr(H_0;
	\alpha)}\frac{\Pr(D|H_1)}{\Pr(D|H_0)} \\
	B(v,v'; \alpha) &= \left(1+{B_{01}(\alpha)}^{-1}\right)^{-1}	
\end{split}
\end{equation*}


\subsection{Combined distance measure}

Using the bayesian probabilities derived in the previous section, we can design
a novel distance measure that expresses the similarity between words whilst
weighing the distributional and semantic information. We propose to
combine semantic and distirbutional information by using the distance measure
$B(v,v';\alpha)$ as a biasing component to the cosine distance in the vector
space of word embeddings:

\begin{equation*}
	\Lambda(v, v'; \alpha, \beta) = \beta B(v,v'; \alpha) +
	(1-\beta)\frac{\mli{dist}(vec(v),vec(v'))}{2}
\end{equation*}

The parameter $\beta$ regulates how much the distributional is weighted in the
distance measurement. The term $\beta B(v,v'; \alpha)$ acts as a bias form the
cosine distance in the sense that it increases the distance between two words,
if their distributional information is contradictory. 
\subsection{Relationship to other distributional measures}

In \cite{baker1998distributional} Baker et al. propose a variant of the
Jensen-Shannon distance to calculate the information loss when merging two
words:

\begin{equation*}
\begin{split}
D_B(v_1, v_2) = &\Pr(v_1) \cdot
\mathbb{D}\left(\Pr(\omega|v_1)\Vert\Pr(\omega|v_1 \lor v_2)\right) + \\  
&\Pr(v_2) \cdot \mathbb{D}\left(\Pr(\omega|v_2)\Vert\Pr(\omega|v_1 \lor v_2)\right)
\end{split}
\end{equation*} 

Here $\mathbb{D}(\cdot \Vert \cdot)$ indicates the Kullback-Leibler divergence. In
\cite{baker1998distributional} it is used to measure the difference of the
distributions of a word being as a single probabilistic event and 
the collapsed event after merging two words. For $\Pr(H_0) = \Pr(H_1) = 0.5$,
we found empirical evidence that $\Pr(H_1|D)$ is proportional their metric (see Figure \ref{fig:jensen-shannon}).
The main advantage of using our method, is the possibility to incorporate prior 
knowledge in $\Pr(H_i), i=0,1$, which is naturally offered by modelling the
problem under a bayesian assumption.

\section{Implementation of the preprocessing step}

As discussed in section \ref{sec:}, we can reduce statistical sparsity
by merging terms that are similar with respect to a distributional and
semantic measure. Up until now we considered the pairwise distance
between word $v \neq v'$. Finding \emph{groups} of similar words, can also be
viewed as a clustering problem.
Let $V = \{v_1,\ldots,v_{|V|}\}$ be a set of terms of a document set $D$ and
$\mathbf{W}=(w_{ij}) \in \mathbb{R}^{|V|\times|V|}$ a symmetrical \emph{dissimilarity matrix}, 
where the elements $w_{ij}$ indicate dissimilarity between the term $v_i$ and
$v_j$ according to our distance function $\Lambda$, i.e.
$w_{ij} = \Lambda(v_i, v_j; \alpha, \beta) = \Lambda(v_j, v_i; \alpha, \beta) =
w_{ji}$. The dissimilarity matrix can then be used as input for a
clustering algorithm to group similar terms together
\cite{baker1998distributional}. In this work we use a greedy agglomerative
hierarchical clustering algorithm. [Clustering algorithmus genau beschreiben oder auf
bekannte Quelle verweisen?]

\subsection{Greedy Agglomarative Hierarchical Clustering}

We start from the assumption that at first (step $i=0$) every word $v \in V$ is
a cluster. At each step $i$ the clusters are merged pairwise into bigger
clusters. The algorithm stops, when there are $K$ clusters left. The criterion
to decide whether to merge two clusters or not is based on the dissimilarity measure 
$\Lambda$ and a \emph{linkage criterion} $\lambda$. At step $i>0$ a 
cluster $C$ is merged with the closest cluster $C' \neq C$, i.e.

\begin{equation*}
 C' = \min\limits_{C'' \in \mathcal{C}_{i-1}} \lambda(C,C'') 
\end{equation*}

where $\mathcal{C}_{i-1}$ denotes the clustering yielded at step $i-1$.
Possible functions that can be chosen for $\lambda$ are e.g. the minimum, the
maximum or the average distance between words in the different clusters. In order to make sure that every word in a
cluster best fits the distributional information represented in the cluster, we
chose the \emph{maximum}, it being the most conservative linkage criterion.  

\begin{equation*}
	\lambda(C,C') = \max\limits_{v \in C, v' \in C' } \Lambda(v,v';\alpha, \beta)
\end{equation*}

Using $\lambda$ as above is also known as \emph{complete-linkage clustering}.
As opposed to \emph{single-linkage}, where two clusters are merged together
based on their closest members, complete-linkage clustering avoids the
so called \emph{chaining phenomeon} - clusters that have many elements
that are very distant to each other might be merged because of their clostest
elements.

The advantage of using agglomerative clustering, is that it allows to operate on
\emph{any} distance function designed by the user. On the other hand, clustering
algorithms like $K$-means or DB-Scan require the actual observations,
which in our case are not available. However, a property shared with $K$-means
is that the number of clusters $K$ has to be chosen by the user of the
algorithms (see Chapter \ref{ch:Evaluation}).

\subsection{Bag-of-clusters}

Let $\mathcal{C} = \left\{ C_1,\ldots,C_K \right\} $ the set of clusters
over $V$, where $C_k \subset V, C_k \cap C_{k'} = \emptyset,~k \neq
k'$. We define the bag-of-clusters of $d \in D$ as  

\begin{equation*}
\begin{split}
	 \mathbf{c}(d) &= (c_1(d),c_2(d),\ldots,c_K(d)) \\
	 c_k(d) &= \sum\limits_{v \in C_k} \mathbf{b}(d,v)
\end{split}
\end{equation*}

Where $\mathbf{b}(d,v)$ indicates the bag-of-words entry for word $v$ in
document $d$ (see Section \ref{sec:formalization}). The $k$-th element of
bag-of-clusters combines the counts of the words in a clique $C_k$. Note that this
the same representation is obtained in bag-of-words, if occurences of a term $v \in C_k$ 
are replaced by any member of the clique $v_k^* \in C_k$ in the documents $d
\in D$. Formally this is

\begin{equation*}
\begin{split}
	d &= (w_1,\ldots,w_{|d|}) \in D \\
	\boldsymbol\sigma(d) &\coloneqq (\sigma(w_1),
	\sigma(w_2),\ldots,\sigma(w_{|d|}))
	\\
	\sigma(w) &= v_k^*, w \in C_k \\ 
	\mathbf{b}(\sigma(d)) &= \mathbf{c}(d)
\end{split}
\end{equation*}

\subsection{Clustering on N-Grams}

When usign $N-Grams$, it is very likely that there is no
corresponding vector representation in the embedding vector space. However, in
order to be able to measure the distance between $N$-Grams, we use the
\emph{maximal distance} between the corresponding unigrams. E.g. the distance
between $v=\text{'new CEO'}$ and $v'=\text{'future Manager'}$ is defined
as

\begin{equation*}
	\mli{dist}_2(v,v') = \max(\mli{dist}(\text{'new'}, \text{{'future'}}),
	\mli{dist}(\text{'CEO'}, \text{'Manager')})
\end{equation*}

More generally we define a separate dissimilarity matrix and compute
different clusteringsfor each $N$-Gram level used in dataset:

\begin{equation*}
\begin{split}
	\mathbf{W}^{(n)} &\in [0,1]^{|V_n|\times|V_n|} \\
	\mathbf{W}^{(n)} &= \left(w^{(n)}_{ij}\right) \\
	w^{(n)}_{ij} &= \max\left\{ \Lambda\left(v_i(k), v_j(k); \alpha,
	\beta\right) \mid k = 1,\ldots,n\right\} \\
	&= dist_n(v_i,v_j), v_i, v_j \in V_n
\end{split}
\end{equation*}

$V_n$ here denotes the vocabulary of $N$-grams with $N=n$ of a document set
$D$ (see Chapter \ref{ch:problem-analysis} Section \ref{sssec:n-grams}). With
$v_i(k)$ we denoted the $k$-th unigram of the $N$-gram $v_i$.
We then cluster the $N$-grams separately, i.e. we obtain a clustering
$\mathcal{C}^{(n)}$ for each level of $N$-grams with $N=n$. The resulting
document set is a concatenation of the bag-of-clusters for the different $N$-grams.

\subsection{Mapping OOV-words}

An unknown document $\tilde{d} \in \mathcal{D}\setminus D$ might contain words not
known by the classifiers, i.e. $\tilde{v} \in \mathcal{V}\setminus V$. Prior to
representing it as a bag-of-clusters, we substitute every word $\tilde{v} \in
\mathcal{V} \setminus V$ with the closest word $v \in V$ with respect to the
cosine distance of the corresponding word vectors. If the closest word is
further away than $\theta_d$ we discard it. The mapping of OOV-words can be
formally described as a proprocessing function $\tau$ as follows:

\begin{equation*}
\begin{split}
	\tilde{d} &= (\tilde{w}_1, \ldots, \tilde{w}_{|\tilde{d}|}) \in
	\mathcal{D}\setminus D \\	
	\boldsymbol\tau(\tilde{d}) &= (\tau(\tilde{w}_1), \ldots,
	\tau(\tilde{w}_{|\tilde{d}|})) \\	
	\tau(\tilde{w}) &= \begin{cases} \tilde{w} & \text{if~} \tilde{w} \in V \\ v' &
	\text{if~} v' = \arg\min\limits_{v''\in V}\mli{dist}(\tilde{w},v'') 
	\text{~and~} \mli{dist}(v',\tilde{w}) \leq \theta_d \\ \epsilon
	& \text{otherwise}
	\end{cases}	
\end{split} 
\end{equation*}

Here $\epsilon$ is the empty word and simply denotes that a word is removed
from the document in case there is no suitable substitute satisfying the
constraint given by the distance threshold.
