\chapter{Method}

In this chapter we describe a method to process a dataset prior to
classification, such that the described problems, namely the statistical
sparsity and OOV-words are reduced. Our approach consists of two main
elements:

\begin{itemize}  
  \item A dissimilarity measure for words based on both distributional and
  semantical information
  \item The usage of a state-of-the-art clutering algorithm to find groups of
  similar words

\end{itemize}

\section{Word Merging}

As mentioned in \ref{sec:echobot}, as a measure to reduce featue sparsity
whilst improving the statistical quality of unfrequent words, our goal
is to find groups of features to merge with each other. 
In order to find good candidates for merging, we consider all the pieces of
information we have about words $v \in V$ in a labeled dataset $L=(D,\omega)$. These are

\begin{enumerate}
  \item the frequencies of the terms with respect to 
  \begin{itemize}
    \item each document  $N(v,d)$
  	\item the whole dataset $N(v)$
  	\item the classes in which they appear $N(v,\omega'), \omega' \in
  	\{+,-\}$
  \end{itemize}  
  \item the pairwise distances in the vector space yielded by a word embeddings
  $\mathcal{W}=(V_{\mathcal{W}}, \mathbf{x}_i)$
\end{enumerate}  

For merged features we require that they are as similar as possible, meaning
that the merging procedure should reflect a semantic understanding of the
relationship between merging candidates.
 

\section{Combining semantic and distributional similarity}

As shown in \ref{sec:echobot}, we can reduce the statistical sparsity of a
BoW-feature vector, by merging similar terms with semantic similarity. However,
we also showed that using only semantic similarity as a merge criterion,
might not yield the desired improvement in classification. This is due to the
fact that semantic similar words might be distributed differently over the
classes. In this section, we will derive a novel distance measure that
integrates both semantic and distributional information. We subdivide the
derivation into two steps: 
First we show how to express the distributional dissimlarity between two words.
We require the distance measure to yield higher distances the more words 
are distributed differently among the categories. Secondly, we address the
question of how to combine the distributional distance and the semantic
dissimilarity into one single metric.

The following considerations are based on distances between \emph{pairs} of words. In
section \ref{sec:clustering} the pairwise distance measure is used to find 
clusters of similar words.

\subsection{Word occurences as a Bernoulli Process}
In order to derive the distributional dissimilarity between pairs of words, it
is useful to view the word occurences as being generated by a probabilistic
process. In the classification tasks considered in this work, we assume that
there are two classes. A commonly used probabilistic model to describe word
occurences is the \emph{bernoulli process}\textbf{[citation]}.\footnote{The 
corresponding distribution that suits a mulitclass scenario is the \emph{multinomial
distribution}.} 

Assume a word $v \in V$ appears $n$ times in a labeled dataset $L=(D,\omega)$.
We model the occurences of $v$ as random variables $Y_1, Y_2, Y_3, \ldots, Y_n$ over the events $\Omega = \{ +, -\}$, 
i.e. $v$ can appear in the positive or the negative class respectively:

\begin{equation*}
	Y_i \coloneqq Y_i(\omega) = \begin{cases} 1 & \mathrm{if~} \omega = \mathrm{+}
	\\
	0 & \mathrm{if~} \omega = \mathrm{-} \end{cases}, i = 1,\ldots,n
\end{equation*}

We also introduce the random variable $X$ that indicates the total
amount of occurences in the positive class:

\begin{equation*}
	X = \sum\limits_{i=1}^n Y_i
\end{equation*}

Then the probability that for $n$ occurences of $v$, the word appears $k$ times
in the positive class is: 

\begin{equation*}
	\Pr(X=k|\theta) = \binom{n}{k} \theta^k(1-\theta)^{n-k}
\end{equation*}

The probability distribution above is also known as \emph{bernoulli or binomial
distribution}. Here $\theta \in [0,1]$ denotes the distribution parameter.
It corresponds to the probability of that a single draw of the word $v$ results in
a occurence in the positive class:

\begin{equation*}
	\Pr(Y_i = 1) = \theta
\end{equation*}

A bernoulli process $X$ with $n$ outcomes  
and parameter $\theta$ is also written as:

\begin{equation*}
	X \sim B(n, \theta)
\end{equation*}

\subsection{Merging words as a probabilistic event}

Now we address the question of how the merger of two words can be described in
the context of the introduced probabilistic bernoulli process. 
When merging two words, we combine their statistics and as a consequence the
occurences of either word collapses to one single event. 
Consider the probability two different words represented by the random
variables $X_1, X_2$ appearing $n_1, n_2$ in the dataset and $k_1, k_2$ times in
the positive class. If we assume that words occurences are indepedent, then we
have the following joint probability:

\begin{equation*}
\begin{split}
	\Pr(X_1 = k_1, X_2 = k_2; \theta_1, \theta_2) &=
	\Pr(X_1=k_1;\theta_1)\Pr(X_2=k_2;\theta_2)
	\\
	&= 	\binom{n_1}{k_1}\theta_1^{k_1}(1-\theta_1)^{n_1-k_1} \cdot
	\binom{n_2}{k_2}\theta_2^{k_2}(1-\theta_2)^{n_2-k_2}
\end{split}
\end{equation*}

When merging the words, we assume that the underlying independent events are
generated by the same bernoulli process. Formally, we have the following hypotheses: 

\begin{enumerate}
  \item $H_0$: The word observations $X_1, X_2$ are generated by two different
  processes, i.e.  generally $\theta_1 \neq \theta_2$
  \item $H_1$: The words observations $X_1, X_2$ are generated by the same
  process, i.e. $\theta_1 = \theta_2$
\end{enumerate}

For the first hypothesis, since we have no further knowledge about the
underlying parameters, we estimate $\theta_i$ to be the maximum likelihood 
estimator based on the observations: 

\begin{equation*}
	H_0: \hat{\theta}_{H_0,i} = \frac{k_i}{n_i}
\end{equation*}

For the second hypothesis, we assume that the indepedent observations $X_1, X_2$
have the same underlying distribution parameter. We express this by combining
the statistics of the observations for $X_1$ and $X_2$:

\begin{equation*}
H_1: \hat{\theta}_{H_1} =\hat{\theta}_{H_1,1} =
\hat{\theta}_{H_1,2} =
\frac{k_1 + k_2}{n_1 + n_2}
\end{equation*}
 
The situation is analogous to observing the outcomes of two different series of
coin tosses $X_1, X_2$ and considering the hypotheses that the two
observations have been generated by two different coins ($\theta_1 \neq \theta_2$) or by the same coin
($\theta_1 = \theta_2$). Given the observed data, our goal is now to calculate
the probability for each hypothesis. This can be done with using bayesian probability theory.
Moreover, since our approach operates on probabilities, the values yielded are
in the interval $[0, 1]$ and similarly scaled as the cosine distance. 

\subsection{Bayesian Hypothesis Testing}

\emph{Bayesian Hypothesis Testing} is a tool to compute the probability of
competing models based on the available observations. In our context, we want to
compare how well the occurences of two words $v_1, v_2$ in a document set $D$ is
explained before and after merging them.
We start from the proposition, that either one hypothesis or the other must be true, i.e.:

\begin{equation*}
	\Pr(H_0|D) + \Pr(H_1|D) = 1
\end{equation*}

The probabilities of the hyptheses $H_i,~i=0,1$, i.e. the words are generated by
two different bernoulli processes or by the same, can be expressed using the
bayesian rule as follows:

\begin{equation*}
\begin{split}
	\Pr(H_0|D) &= \frac{\Pr(D|H_0)\Pr(H_0)}{\Pr(H_0)\Pr(D|H_0) + \Pr(H_1)\Pr(D|H_1)} \\
			 & =\frac{1}{1+
			 \underbrace{\frac{\Pr(H_1)}{\Pr(H_0)}\frac{\Pr(D|H_1)}{\Pr(D|H_0)}}_{
			 \eqqcolon B_{01}}} = (1+B_{01})^{-1} \\
    \Pr(H_1|D) &= 1 - (1 + B_{01})^{-1}= \left(1+{B_{01}}^{-1}\right)^{-1}
\end{split}
\end{equation*}

Here $B_{01}$ is called the \emph{Bayes Factor} and is often used as an
alternative to the probability to express the difference between two competing hypotheses.
Using the probability distribution function introduced  in the previous
section we obtain: 

\begin{equation*}
\begin{split}
\Pr(D|H_0) &= \Pr\left(X_1=k_1, X_2=k_2 \mid \theta_1 = \hat{\theta}_{H_0,1},
\theta_2 = \hat{\theta}_{H_0,2}\right) \\
&=
\binom{n_1}{k_1}\hat{\theta}_{H_0,1}^{k_1}\left(1-\hat{\theta}_{H_0,1}\right)^{n_1-k_1}
\cdot
	\binom{n_2}{k_2}\hat{\theta}_{H_0,2}^{k_2}\left(1-\hat{\theta}_{H_0,2}\right)^{n_2-k_2} 
\\
\Pr(D|H_1) &= \Pr\left(X_1=k_1, X_2=k_2 \mid \theta_1 = \theta_2 = \frac{k_1 +
k_2}{n_1 + n_2}\right) \\
&= \binom{n_1}{k_1}\binom{n_2}{k_2}\hat{\theta}_{H_1}^{k_1 +
k_2}\left(1-\hat{\theta}_{H_1}\right)^{n_1 + n_2 - k_1 - k_2}
\end{split}
\end{equation*}

We then insert $P(D|H_i),~i=0,1$ in the bayes factor $B_{01}$ and obtain:

\begin{equation*}
B_{01} = \frac{\Pr(H_0)}{\Pr(H_1)} \cdot
\frac{\hat{\theta}_{H_0,1}^{k_1}\left(1-\hat{\theta}_{H_0,1}\right)^{n_1-k_1}\hat{\theta}_{H_0,2}^{k_2}\left(1-\hat{\theta}_{H_0,2}\right)^{n_2-k_2}
}{\hat{\theta}_{H_1}^{k_1 + k_2}\left(1-\hat{\theta}_{H_1}\right)^{n_1 +
n_2 - k_1 - k_2}}
\end{equation*}

\subsection{Integrating external knowledge in the priors}
When there is no knowledge about the prior probabilities $\Pr(H_0), \Pr(H_1)$ a
common heuristic is to assume that every hypothesis is equally probable, i.e.
$\Pr(H_0) = \Pr(H_1) = 0.5$ \textbf{[citation]}. However, we can use the information
provided by word embeddings to approximate the priors. The calculation of word
vectors implies predicting the probability of a word appearing in a given
context. The cosine similarity between two word vectors, can be interpreted as
an approximation of the probability that two words $v_1, v_2$ in question appear
in the same context $C$ \textbf{[citation needed]}. 

\begin{equation*}
	\cos \angle (vec(v_1), vec(v_2)) \sim \Pr(\text{``}v_1, v_2~\text{appear in
	similar contexts} \text{``})
\end{equation*}

We can use this interpretation of the cosine similarity to incorporate the
knowledge captured by word embeddings in the priors. We propose to use: 

\begin{equation*}
\begin{split}
	\Pr(H_0; \alpha) &= \alpha\cdot\left(\frac 1 2 - \frac{dist(v_1,v_2)}{2}\right)
	+ \frac 1 2
	\\
	\Pr(H_1; \alpha) &= 1 - \Pr(H_0)
\end{split}
\end{equation*}

For words that are farther away in the emebedding vector space, we want to
favour the first hypothesis, i.e. that the words should not be merged. When
words are closer they tend to appear in similar contexts, which is a hint that
the two words can be merged without losing discriminative information. $\alpha$
is a linear weight that regulates how much the cosine distance affects the
priors to deviate from $0.5$. For $\alpha=0$ the cosine similarity is completely
negletcted and the hypotheses are assumed to be equally probable. For $\alpha >
0$, the priors deviate from $0.5$ proportionally to $\alpha \cdot
\frac{\mli{dist}(v_1,v_2)}{2}$.\footnote{In our formula we use
$\mli{dist}(v_1,v_2)/2$ in order to normalize the cosine distance to the interval $[0, 1]$}

Using the hypothesis probability $\Pr(H_1|D)$ we define a
distributional distance measure between two words $v, v' \in V$ as follows:

\begin{equation*}
\begin{split}
	B_{01}(\alpha) &= \frac{\Pr(H_0; \alpha)}{1-\Pr(H_0;
	\alpha)}\frac{\Pr(D|H_1)}{\Pr(D|H_0)} \\
	B(v,v'; \alpha) &= \left(1+{B_{01}(\alpha)}^{-1}\right)^{-1}	
\end{split}
\end{equation*}


\subsection{Combined distance measure}

Using the bayesian probabilities derived in the previous section, we can design
a novel distance measure that expresses the similarity between words whilst
weighing the distributional and semantic information. We propose to
combine semantic and distirbutional information by using the distance measure
$B(v,v';\alpha)$ as a biasing component to the cosine distance in the vector
space of word embeddings:

\begin{equation*}
	\Lambda_{\alpha,\beta}(v, v') = \beta B(v,v'; \alpha) +
	(1-\beta)\frac{\mli{dist}(vec(v),vec(v'))}{2}
\end{equation*}

The parameter $\beta$ regulates how much the distributional is weighted in the
distance measurement. The term $\beta B(v,v'; \alpha)$ acts as a bias form the
cosine distance in the sense that it increases the distance between two words,
if their distributional information is contradictory. 
\subsection{Relationship to other distributional measures}

In \cite{baker1998distributional} Baker et al. propose a variant of the
Jensen-Shannon distance to calculate the information loss when merging two
words:

\begin{equation*}
\begin{split}
D_B(v_1, v_2) = &\Pr(v_1) \cdot
\mathbb{D}\left(\Pr(\omega|v_1)\Vert\Pr(\omega|v_1 \lor v_2)\right) + \\  
&\Pr(v_2) \cdot \mathbb{D}\left(\Pr(\omega|v_2)\Vert\Pr(\omega|v_1 \lor v_2)\right)
\end{split}
\end{equation*} 

Here $\mathbb{D}(\cdot \Vert \cdot)$ indicates the Kullback-Leibler divergence. In
\cite{baker1998distributional} it is used to measure the difference of the
distributions of a word being as a single probabilistic event and 
the collapsed event after merging two words. For $\Pr(H_0) = \Pr(H_1) = 0.5$,
we found empirical evidence that $\Pr(H_1|D)$ is proportional their metric (see Figure \ref{fig:jensen-shannon}).
The main advantage of using our method, is the possibility to incorporate prior 
knowledge in $\Pr(H_i), i=0,1$, which is naturally offered by modelling the
problem under a bayesian assumption.

\section{Implementation of the preprocessing step}

As discussed in section \ref{sec:}, we can reduce statistical sparsity
by merging terms that are similar with respect to a distributional and
semantic measure. Up until now we considered the pairwise distance
between word $v \neq v'$. Finding \emph{groups} of similar words, can also be
viewed as a clustering problem.
Let $V = \{v_1,\ldots,v_{|V|}\}$ be a set of terms of a document set $D$ and
$\mathbf{W}=(w_{ij}) \in \mathbb{R}^{|V|\times|V|}$ a symmetrical \emph{dissimilarity matrix}, 
where the elements $w_{ij}$ indicate dissimilarity between the term $v_i$ and
$v_j$ according to our distance function $\Lambda$, i.e.
$w_{ij} = \Lambda(v_i, v_j; \alpha, \beta) = \Lambda(v_j, v_i; \alpha, \beta) =
w_{ji}$. The dissimilarity matrix can then be used as input for a
clustering algorithm to group similar terms together
\cite{baker1998distributional}. In this work we use a greedy agglomerative
hierarchical clustering algorithm. 

\subsection{Greedy Agglomarative Hierarchical Clustering}

We start from the assumption that at first (step $i=0$) every word $v \in V$ is
a cluster. At each step $i$ the clusters are merged pairwise into bigger
clusters. The algorithm stops, when there are $K$ clusters left. The criterion
to decide whether to merge two clusters or not is based on the dissimilarity
matrix $\mathbf{W} = (w_{ij})$ and a \emph{linkage criterion} $\lambda$. At
step $i>0$ a cluster $C$ is merged with the closest cluster $C' \neq C$, i.e.

\begin{equation*}
 C' = \min\limits_{C'' \in \mathcal{C}_{i-1}} \lambda(C,C'') 
\end{equation*}

where $\mathcal{C}_{i-1}$ denotes the clustering yielded at step $i-1$.
Possible functions that can be chosen for $\lambda$ are e.g. the minimum, the
maximum or the average distance between words in the different clusters. In order to make sure that every word in a
cluster best fits the distributional information represented in the cluster, we
chose the \emph{maximum}, it being the most conservative linkage criterion.  

\begin{equation*}
	\lambda(C,C') = \max\limits_{v_i \in C, v_j \in C' } w_{ij} 
\end{equation*}

When the algorithm stops it outputs a clustering $\mathcal{C} = \left\{ C_1,\ldots,C_K \right\} $ 
over $V$, where $C_k \subset V, C_k \cap C_{k'} = \emptyset,~k \neq
k'$. 

Using $\lambda$ as above is also known as \emph{complete-linkage clustering}.
As opposed to \emph{single-linkage}, where two clusters are merged together
based on their \emph{closest} members, complete-linkage clustering avoids the
so called \emph{chaining phenomeon}, i.e. clusters that have many elements
that are very distant to each other might be merged because of their clostest
elements.

The advantage of using agglomerative clustering, is that it allows to operate on
a dissimilarity matrix based on \emph{any} distance function designed by the
user. On the other hand, clustering algorithms like $K$-means or DB-Scan require
the actual observations, which in our case are not available. However, a property 
shared with $K$-means is that the number of clusters $K$ has to be chosen by the
user of the algorithms (see Chapter \ref{ch:Evaluation}).

\subsection{Bag-of-clusters}

After finding clusters of similar words, we preprocess each document by
representing it as a \emph{bag-of-clusters}. For a document in a dataset with
vocabulary $V$ a bag-of-clusters is defined as

\begin{equation*}
\begin{split}
	 \mathbf{c}(d) &= (c_1(d),c_2(d),\ldots,c_K(d)) \\
	 c_k(d) &= \sum\limits_{v \in C_k} \mathbf{b}(d,v)
\end{split}
\end{equation*}

where $\mathbf{b}(d,v)$ indicates the bag-of-words entry for word $v$ in
document $d$ (see Section \ref{sec:formalization}). The $k$-th element of
bag-of-clusters combines the counts of the words in a clique $C_k$. Note that this
the same representation is obtained by representing the document as
a BoW, if in every document $d \in D$ the occurences of a term $v \in C_k$
are replaced by any member of the clusters $v_k^* \in C_k$ . Formally this is

\begin{equation*}
\begin{split}
	d &= (w_1,\ldots,w_{|d|}) \in D \\
	\boldsymbol\sigma(d) &\coloneqq (\sigma(w_1),
	\sigma(w_2),\ldots,\sigma(w_{|d|}))
	\\
	\sigma(w) &= v_k^*, w \in C_k \\ 
	\mathbf{b}(\sigma(d)) &= \mathbf{c}(d)
\end{split}
\end{equation*}

\subsection{Clustering on N-Grams}

When usign $N-Grams$, it is very likely that there is no
corresponding vector representation in the embedding vector space. However, in
order to be able to measure the distance between $N$-Grams, we use the
\emph{maximal distance} between the corresponding unigrams. E.g. the distance
between $v=\text{'new CEO'}$ and $v'=\text{'future Manager'}$ is defined
as

\begin{equation*}
	\mli{dist}_2(v,v') = \max(\mli{dist}(\text{'new'}, \text{{'future'}}),
	\mli{dist}(\text{'CEO'}, \text{'Manager')})
\end{equation*}

More generally we define a separate dissimilarity matrix and compute
different clusteringsfor each $N$-Gram level used in dataset:

\begin{equation*}
\begin{split}
	\mathbf{W}^{(n)} &\in [0,1]^{|V_n|\times|V_n|} \\
	\mathbf{W}^{(n)} &= \left(w^{(n)}_{ij}\right) \\
	w^{(n)}_{ij} &= \max\left\{ \Lambda\left(v_i(k), v_j(k); \alpha,
	\beta\right) \mid k = 1,\ldots,n\right\} \\
	&\eqqcolon \Lambda_n(v_i,v_j;\alpha,\beta), v_i, v_j \in V_n
\end{split}
\end{equation*}

$V_n$ here denotes the vocabulary of $N$-grams with $N=n$ of a document set
$D$ (see Chapter \ref{ch:problem-analysis} Section \ref{sssec:n-grams}). With
$v_i(k)$ we denoted the $k$-th unigram of the $N$-gram $v_i$.
We then cluster the $N$-grams separately, i.e. we obtain a clustering
$\mathcal{C}^{(n)}$ for each level of $N$-grams with $N=n$. The resulting
document set is a concatenation of the bag-of-clusters for the different $N$-grams.

\subsection{Mapping OOV-words to known Vocabulary}

An unknown document $\tilde{d} \in \mathcal{D}\setminus D$ might contain words not
known by the classifiers, i.e. $\tilde{v} \in \mathcal{V}\setminus V$. Prior to
representing it as a bag-of-clusters, we substitute every word $\tilde{v} \in
\mathcal{V} \setminus V$ with the closest word $v \in V$ with respect to the
cosine distance of the corresponding word vectors. If the closest word is
further away than $\theta_d$ we discard it. The mapping of OOV-words can be
formally described as a proprocessing function $\tau$ as follows:

\begin{equation*}
\begin{split}
	\tilde{d} &= (\tilde{w}_1, \ldots, \tilde{w}_{|\tilde{d}|}) \in
	\mathcal{D}\setminus D \\	
	\boldsymbol\tau(\tilde{d}) &= (\tau(\tilde{w}_1), \ldots,
	\tau(\tilde{w}_{|\tilde{d}|})) \\	
	\tau(\tilde{w}) &= \begin{cases} \tilde{w} & \text{if~} \tilde{w} \in V \\ v' &
	\text{if~} v' = \arg\min\limits_{v''\in V}\mli{dist}(\tilde{w},v'') 
	\text{~and~} \mli{dist}(v',\tilde{w}) \leq \theta_d \\ \epsilon
	& \text{otherwise}
	\end{cases}	
\end{split} 
\end{equation*}

Here $\epsilon$ is the \emph{empty word} and simply denotes that a word is removed
from the document in case there is no suitable substitute satisfying the
constraint given by the distance threshold.

\subsection{Preprocessing}

The preprocessing of a dataset involves finding clusters using the distance
measure introduced in Section \ref{sec:distance-measure} and then represent each
document as a bag-of-clusters. The actual classification process involves
building a classification model with preprocessed training dataset and prepare
unknown documents for prediction by mapping OOV-words to known vocabulary. The
following sections go into the details of the mentioned preprocessing steps.  

\subsubsection{Preprocessing the training data}

Listing \ref{alg:preprocess-training-data} shows a qualititative algorithm of
how we intend to preprocess the training data prior to building a
classification model. 

\begin{algorithm}

\label{alg:preprocess-training-data}
\caption{Algorithm structure for preprocessing training data}
\begin{algorithmic}[1]
\Function{PreprocessTrainingData}{$\protect D$, $\protect N$, $\protect
\boldsymbol \alpha[1..N]$, $\protect
\boldsymbol \beta[1..N]$, $\protect \boldsymbol K[1..N]$}
\label{alg:preprocess-training-data:parameters}
\State $D_N^* \gets g_N^*(D)$ \Comment{Append $2,\ldots,N$-Grams to each
document}
\label{alg:preprocess-training-data:n-grams}
\State $V_N^* \coloneqq \mli{voc}(D_N^*)$
\State $V_N^* \gets V_1 \cup V_2 \cup \ldots \cup V_N$
\State $\mathcal{C} \coloneqq \emptyset$
\For{$n \in \left\{1,\ldots,N\right\}$}
\label{alg:preprocess-training-data:loop}
\State $\mathbf{W}^{(n)} \coloneqq \mli{pairwise-distances}(V_n,
\Lambda_n^{\boldsymbol \alpha[n], \boldsymbol \beta[n]})$

\State $\mathcal{C}^{(n)} \coloneqq
\mli{complete-linkage-cluster}(\mathbf{W}^{(n)}, \boldsymbol K[n])$
\State $\mathcal{C} \gets \mathcal{C} \cup \mathcal{C}^{(n)}$
\EndFor
\State $\mathbf{F} \gets \mli{bag-of-clusters}(D_N^*, \mathcal{C})$
\label{alg:preprocess-training-data:boc} 
\Comment{The final feature matrix used for training}
\State \Return $(\mathcal{C}, V_N^*, \mathbf{F})$
\EndFunction
\end{algorithmic}
\end{algorithm}

The preprocessing step accepts a dataset $D$, the size of N-Grams to be used
$N$, metric parameters $\boldsymbol \alpha, \boldsymbol \beta$ and a clustering
parameter $\boldsymbol K$ (Line \ref{alg:preprocess-training-data:parameters}).
Each $N$-Gram level is parametirized with its own parameter set, i.e. for
$N$-grams of size $N=n$ we use parmeters $\boldsymbol\alpha[n],
\boldsymbol\beta[n]$ and $\boldsymbol K[n]$ for the distance metric $\Lambda_n$
and the clustering algorithm respectively. In Line
\ref{alg:preprocess-training-data:n-grams} the document is affixed with
$N$-Grams of size $1,\ldots,N$. In the loop on line
\ref{alg:preprocess-training-data:loop} we iterate over the different
sizes of $N$-grams ($n=1,\ldots,n$) and calculate the respective dissimilarity
matrix based on $\Lambda_n$ and the a complete-linkage clustering with $\boldsymbol K[n]$ clusters. 
All clusterings are accumulated in $\mathcal{C}$ and finally used to represent
the document as a bag-of-clusters (Line \ref{alg:preprocess-training-data:boc}). The matrix
$\mathbf{F}$ is the resulting feature matrix of the type
$\mathbb{N}^{|D|\times|\mathcal{C}|}$ (number documents $\times$ number of clusters) that is later used for training a
classifier. Each row of the matrix represents a document and each column the
number of occurences of the members of the corresponding cluster in
$\mathcal{C}$.

Using a different parameter set for each $N$-gram level allows adapt the
parameters in order to account for the different distributional properties of
higher order $N$-grams: With $N$ growing the features become more sparse and a
heavier feature reduction (smaller $K$) might be needed to compensate the
sparsity. $\boldsymbol \alpha, \boldsymbol \beta$ regulate how much
the semantical information is weighed against the distributional information in
the distance measurement function (see Section \ref{}). For higher $N$-grams it
might be necessary to increase the weights in favour of semantic information, since the counts
become lower and thus less reliable for probability estimation. 

\subsubsection{Preprocessing unknown Documents}

After having built a classification model using the preprocessed training data,
we want to use it to predict classes of unknown documents. Prior to prediction,
unknown documents have to mapped to feature vectors $\mathbf{\tilde{f}} \in
\mathbb{N}^{|\mathcal{C}|}$. Also, since they might contain words not known by
the classifier, a mapping procedure to known vocabulary has to be performed. Listing
\ref{alg:preprocess-unknown-data} shows a possible algorithm to preprocess
unknown documents to achieve the goals mentioned above.

\begin{algorithm}
\label{alg:preprocess-unknown-data}
\caption{Preprocessing of unknown documents}

\begin{algorithmic}[1]
\Function{PreprocessUnknownDocuments}{$\protect \tilde{D}$, $N$, $V_N^*$,
$\boldsymbol\theta_d[1..N]$, $\mathcal{C}$}

\State $\tilde{D}_N^* \coloneqq g_N^*(\tilde{D})$ 
\label{alg:preprocess-unknown-data:n-grams}
\State $\tilde{V}_N^* \coloneqq \mli{voc}(\tilde{D}_N^*)$

\State $V_N^* \gets V_1 \cup V_2 \cup \ldots \cup V_N$ 
\label{alg:preprocess-unknown-data:n-grams-known-partition}
\State $\tilde{V}_N^* \gets \tilde{V}_1 \cup \tilde{V}_2 \cup \ldots \cup
\tilde{V}_N$
\label{alg:preprocess-unknown-data:n-grams-unknown-partition}

\For{$n \in \{1,\ldots,N\}$}
\State $\tilde{V} \coloneqq \tilde{V}_n \setminus V_n$
\State $\mli{NN} \coloneqq \mli{nearest-neighbor}(\tilde{V}, V_n,\mathcal{W},
\mli{dist}_n) \subset \tilde{V} \times V_n \times [0,1]$ 
\label{alg:preprocess-unknown-data:nearest-neighbor}
\State $S \coloneqq \left\{(\tilde{v}, v) \mid (\tilde{v}, v,d) \in \mli{NN}
\land d \leq \boldsymbol \theta_d[n]\right\}$
\label{alg:preprocess-unknown-data:substitutes} 
\State $R \coloneqq \left\{(\tilde{v}, v)\mid (\tilde{v}, v,d) \in \mli{NN}
\land d > \boldsymbol \theta_d[n]\right\}$ 
\State $\tilde{D}_N^* \gets \mli{substitute}(\tilde{D}_N^*,S)$
\State $\tilde{D}_N^* \gets \mli{remove}(\tilde{D}_N^*,R)$
\label{alg:preprocess-unknown-data:substitutes-end}
\EndFor

\State $\mathbf{\tilde{F}} \coloneqq \mli{bag-of-clusters}(\tilde{D}_N^*, 
\mathcal{C})$ 
\label{alg:preprocess-unknown-data:boc}
\State \Return $\mathbf{\tilde{F}}$
\EndFunction
\end{algorithmic}
\end{algorithm}

The algorithms input is a set of unknown documents $\tilde{D}$, the $N$-gram
size $N$, distance thresholds $\boldsymbol \theta_d[1..N]$ and a clustering
$\mathcal{C}$.

We first transform the set of unknown documents to its $N$-grams representation
(Line \ref{alg:preprocess-unknown-data:n-grams}). In lines
\ref{alg:preprocess-unknown-data:n-grams-known-partition} and 
\ref{alg:preprocess-unknown-data:n-grams-unknown-partition} we partition the known
vocabulary and the vocabulary of unknown documents into separate sizes of $N$-Grams.
 We then iterate through the
sizes of $N$-Grams and calculate the nearest neighbors of the OOV-words
$\tilde{V}$ of the corresponding $N$-Grams of size $N=n$. In line
\ref{alg:preprocess-unknown-data:nearest-neighbor}, based on the word embeddings
$\mathcal{W}$ and the distance measure $\mli{dist}_n$, the function $\mli{nearest-neighbor}$ returns triples 
of the form (OOV word $\tilde{v}$, nearest known word $v$, cosine distance
between $v$ and $\tilde{v}$). 
From this set we filter out the substitutions $S$ that satisfy the
threshold $\boldsymbol \theta_d[n]$ and the ones who do not $R$. The
string functions $\mli{substitute}$ and $\mli{remove}$ are used to substitute or
remove the words from the document set respectively (see lines
\ref{alg:preprocess-unknown-data:substitutes} --
\ref{alg:preprocess-unknown-data:substitutes-end}). In line
\ref{alg:preprocess-unknown-data:boc} the bag-of-clusters
representation of the preprocessed unknown document set is computed and returned
as feature matrix $\mathbf{\tilde{F}}$.

\subsubsection{Classification and prediction}

After defining how to preprocess raw textual data with the method proposed in
this work, we show how the preprocessed data can be incorporated in a 
classification process (see Listing \ref{alg:classification}).

\begin{algorithm}
\label{alg:classification}
\caption{Build classification model and predict class of unknown documents}

\begin{algorithmic}[1]
\Function{BuildModelAndPredict}{$D$, $L$,
$N$, $\protect \boldsymbol \alpha[1..N]$, $\protect
\boldsymbol \beta[1..N]$, $\protect \boldsymbol K[1..N]$,
$\boldsymbol\theta_d[1..N]$}

\State $(\mathcal{C}, V_N^*, \mathbf{F}) \coloneqq
\textproc{PreprocessTrainingData}(D,L,N,\boldsymbol \alpha, \boldsymbol \beta, \boldsymbol K)$ \State $M = \mli{build-classification-model}(\mathbf{F},L)$
\While{Fetch unknown docments $\tilde{D}$}
\State $\mathbf{\tilde{F}} = \textproc{PreprocessUnknownDocuments}(\tilde{D}, N,
V_N^*, \boldsymbol\theta_d, \mathcal{C})$
\State $p = M.\mli{predict}(\mathbf{\tilde{F}})$
\EndWhile

\EndFunction
\end{algorithmic}
\end{algorithm}

The feature matrix returned by \textproc{PreprocessTrainingData} is used for
building a classification model $M$. As a data source continously provides
batches of unknown documents, they are preprocessed by reusing the clustering
computed in while preprocessing the training data.

\section{Complexity}

\section{Summary}