% % LaTeX2e class for student theses % sections/content.tex % % Karlsruhe
% Institute of Technology % Institute for Program Structures and Data
% Organization % Chair for Software Design and Quality (SDQ) % % Dr.-Ing. Erik
% Burger % burger@kit.edu % % Version 1.3, 2016-12-29

\chapter{Introduction}
\label{ch:Introduction}

With billions of users generating textual data every day, the World
Wide Web has undisputedly become the most valuable and rich source
of information when it comes to events with political, economical and
societal relevance. As a matter of fact, this value has been recognized by
many companies and institutions. E.g., \emph{Global Pulse} \cite{globalpulse},
an initiative of the United Nations, leverages textual data produced in social networks to predict job
losses and spending reductions in given regions.
\emph{Google Flu Trends} and \emph{Google Dengue Trends} \cite{googlefludengue},
two projects started by \emph{Google}, estimate the development of Flu and Dengue fever based on \emph{Google} search
patterns. The problem that has to be solved in order to
be able to do so is called \emph{Text Classification} or \emph{Text Categorization(TC)}. This involves
assigning a text document to one ore more predefined \emph{classes} or \emph{categories}. 
Other applications of TC, amongst many more, include sentiment analysis, word
sense disambiguation, spam filtering and document orgnanization \cite{sebastiani2002machine}. 

\section{Motivation}
\label{sec:Motivation}
Nowadays TC is mainly performed using \emph{machine learning (ML)} algorithms.
Generally speaking the strength of ML based classifiers lies in the fact
that they can automatically induce complex rules for classification by
analyzing samples of \textit{preclassified} or \textit{labeled} documents. 
The more labeled samples a classifier is presented with, the better it will be
able to internalize a model able to predict categories of unknown samples. 
Hence, rather than being solely a result of the classification algorithm itself,
the success of ML approaches mostly comes from the quantity of labeled
data used for training. Unfortunately, in most cases the vast majority of
data is unlabeled, and labelling it is a costly and time consuming process. 
As opposed to big corporations, like \emph{Google} or \emph{facebook}, which can
easily afford to allocate the resources required to label large amounts of data, 
small to medium-sized businesses can only do so on a much smaller scale. 

Since nowadays machine learning based text categorization is widely spread
among many areas of industry, being able to analyze and categorize natural
language finds many applications and has become a very valuable skill. 
Therefore, it is this work's purpose to explore the possibility to improve 
the quality of text classification despite the scarcity of labeled data.

\section{Problem Statement}

Inspite of their effectiveness, it is noteworthy that most machine learning
algorithms used in TC have no notion of the semantics behind the words
contained in documents they classify: a word is usually represented as a dimension
in a vector space with hundreds of thousands of other dimensions. E.g in
probabilistic classifiers, such as Naive Bayes, it is the word statistics
generated by the large number of preclassified documents that provide the information relevant to the
underlying classification task.
As a consequence, classifiers trained with too few data samples lack of
robust statistical information, which directly impacts the classification
effectiveness. Also, due to the high complexity of natural language,
unknown documents are very likely to contain words not covered by the
classification model --- a.k.a. \emph{out of vocabulary (OOV)} words.

By contrast, the way how humans categorize text is heavily based on the
knowledge about semantic relationships between words. It is this internal representation
of language that allows humans to classify text without first reading thousands of example documents. 
With this in mind we could ask the question: Is it possible to extend ML
based TC by an \emph{exogenous} knowledge source about word semantics in order
to compensate the lack of the labeled samples?

\section{Proposed Method}
\label{sec:introduction:proposed-method}
\subsection{Word Embeddings}
In recent years Milokov et al. introduced \emph{word emebddings}
\cite{mikolov2013distributed}, a technique to represent words as vectors in a continuous low dimensional vector space. 
A particular property of this approach is that the angle
between two word vectors is a measure of how dissimilar the respective words are from a
semantical point of view.
Moreover, it is possible to answer analogy questions of the form ``\emph{woman} is to
 \emph{queen} as \emph{man} is to?'' by simply computing $\mathit{queen} - \mathit{woman} +
\mathit{man} \approx \mathit{king}$ in the embedding vector space. 

We propose to use the semantic information encoded in word embeddings and the
induced distance metric to improve classification accuracy inspite of the scarcity of labeled
data. The following examples give a general idea of the intended solution.

\subsection{Term Substitutions}
Assume we want to classify movie reviews based on whether they
express approval or disapproval towards the film in question. For the sake of
simplicity, let's assume that our training set contains only four samples, two
for each class:

\begin{itemize}
  \item Positive samples
  \begin{itemize}
	  \item \emph{A good movie.}
	  \item \emph{Just excellent.}
  \end{itemize}
  
  \item Negative samples
  \begin{itemize}
	  \item \emph{As predictable as could be.}
	  \item \emph{An outstanding example of boring.} 
  \end{itemize}
\end{itemize}

The adjectives used in the dataset are distributed as follows:

\begin{center}
\begin{tabular}{l|l|l}
\textbf{Word} & \textbf{Positive Class} & \textbf{Negative Class}  \\
\hline
boring & 0 & 1 \\
excellent & 1 & 0 \\
good & 1 & 0 \\
outstanding & 0 & 1 \\
predictable & 0 & 1 \\

\end{tabular}
\end{center}

The statistics of the dataset might suggest that the word ``outstanding'' is an
indicator for a negative review. However, since it is used in a rather ironic
way, the document containing it might represent an outlier in its class. A
simple approach to overcome this problem is to use the word similarity
in the word vector space to merge words with high semantic similarity, e.g. by
computing the pairwise word cosine distance in the emebedding word vector space.

In this example one could perform the following substitutions

\begin{center}
\begin{tabular}{l|l|l}
\textbf{Word} & \textbf{Substitute with}  \\
\hline
boring & predictable \\
excellent & good \\
outstanding & good \\
\end{tabular}
\end{center}

After the substitution, we obtain the following word statistics:

\begin{center}
\begin{tabular}{l|l|l}
\textbf{Word} & \textbf{Positive Class} & \textbf{Negative Class}  \\
\hline
boring & 0 & 2 \\
good & 2 & 1 \\
\end{tabular}
\end{center}

Since the word ``outstanding'' has been replaced with ``good'', its
statistical bias towards the negative class has been smoothed away by the
occurences in the positive class of its synonyms ``good'' and ``excellent''.
 
In the same way, OOV words could be mapped to words known by the
classifier. Assume the task is to predict the class of the sentence ``Fantastic movie''. 
Using the similarity measure provided by word embeddings one could substitute
the OOV word ``Fantastic'' with ``excellent'' or ``good'' and thus provide the
classifier with important information for the underlying classification task.

\subsection{Distributional Information}
When substituting terms, relying on the semantic similarity between
words might not always yield the desired effect on classification
accuracy. As a matter of fact, the decision of whether to substitute
a word with a semantically similar word depends on the underlying
classification task.
E.g, assume a TC task in which the goal is discriminate good from enthusiastic
movie reviews. Let the word frequencies for an hypothetical training dataset be the following:

\begin{center}
\begin{tabular}{l|l|l}
\textbf{Word} & \textbf{Positive Class} & \textbf{Negative Class}  \\
\hline
excellent & 10 & 3 \\
good & 1 & 10 \\
\end{tabular}
\end{center}

In this case, even though the semantic similarity of the words would favour a
substitution, the \emph{distributional information} strongly suggests otherwise:
the terms are distributed differentely among the classes and substituting ``good'' with
``excellent'' would result in a loss of information that is relevant to 
discrimination. 
This phenomenon can be observed in many classification tasks and
should not be neglected. Therefore, in our method we also propose to use 
the information provided by word distributions as a criterion for good term 
replacements. 

\section{Contributions}
\label{sec:contributions}
The main contributions of this work are the following:

\begin{itemize}
  \item We derive a new \textbf{distributional word distance measure}.
   
  \item Using a probabilistic framework, both the semantic similarity
  measure of word vectors and our distributional similarity are \textbf{combined into a novel
  word distance measure}.
  
  \item A method is provided to use our similarity in connection with a state of
  the art clustering algorithm to \textbf{preprocess textual data} prior to
  classification.

  \item We show the our method allows to \textbf{improves classification
  accuracy} in cases in which labeled data is scarce
\end{itemize}

As opposed to approaches described in \cite{wang2016semantic,
baker1998distributional}, which either used semantic similarity or distributional similarity 
to perform term substitutions with no or little effect on classification
accuracy, the novelty of our approach lies in the fact that \emph{both} measures
are used in combination. Moreover, by doing so we show that our method
outperforms state of the art approaches in various classification tasks. 

We argue that especially when labeled data is scarce, the
improvement in classification accuracies using our method should be all the more evident. Moreover, since our
approach reduces the statistical bias due to the lack of enough training
samples, our method opens the possibility to use features with higher information content, such as \emph{$N$-Grams} 
and \emph{PoS-Tags} (see Chapter \ref{ch:problem-analysis}), even when it would normally result in 
highly sparse feature vectors. This can allow to achieve improvements in TC
even with larger datasets in the future.

\section{Structure}
In Section \ref{sec:formalization} text classification is introduced formally,
whilst establishing a framework of terms and notations that will be used in the course of this work. 
Subsequently, the problems our solution intends to solve are analyzed in more
detail by introducing a real world scenario, in which a similar solution approach is successfully applied by a human agent (Section \ref{sec:echobot}).
Moreover, the statistical bias of unfrequent words and OOV words, will be
formally described and analyzed in Section \ref{sec:statistics}. After deriving it from a 
theoretical point of view in Chapter \ref{ch:implementation}, the effectiveness
of the proposed method, will be evaluated experimentally using state of the art classifiers 
such as Naive Bayes and SVM (see Chapter \ref{ch:Evaluation}). The datasets used
for evaluation are both German datasets, provided by \emph{Echobot GmbH} (see Section \ref{sec:echobot}), 
and publicly available English datasets, that have been used for benchmark in numerous previous research. 
 