% % LaTeX2e class for student theses % sections/content.tex % % Karlsruhe
% Institute of Technology % Institute for Program Structures and Data
% Organization % Chair for Software Design and Quality (SDQ) % % Dr.-Ing. Erik
% Burger % burger@kit.edu % % Version 1.3, 2016-12-29

\chapter{Introduction}
\label{ch:Introduction}

With billions of users generating textual data every day, the World
Wide Web has undisputedly become the most valuable and rich source
of information when it comes to events with political, economical and
sociological relevance. As a matter of fact, this value has been recognized by
many companies and institutions. \emph{Global Pulse}, an initiative of the United
Nations, leverages textual data produced in social networks, to predict job
losses and spending reductions in given regions. \emph{Google Flu
Trends} and \emph{Google Dengue Trends}, two projects started by \emph{Google},
estimate the development of Flu and Dengue fever based on \emph{Google} search
patterns. The problem that has to be solved in order to be able to do so is
called \emph{Text Classification} or \emph{Text Categorization(TC)}. This involves
assigning a text document to one ore more predefined \emph{classes} or \emph{categories}. 
Other applications of TC, amongst many more, include sentiment analysis, word
sense disambiguation, spam filtering and document orgnanization. 

\section{Motivation}
\label{sec:Motivation}
Nowadays, TC is mainly performed using \emph{machine learning (ML)} algorithms.
Generally speaking, the strength of ML based classifiers lies in the fact
that they can automatically induce complex rules for classification, by
analyzing samples of \textit{preclassified} or \textit{labeled} documents. 
The more labeled samples a classifier is presented with, the better it will be
able to internalize a model able to predict categories of unknown samples. 
Hence, rather than being solely a result of the classification algorithm itself,
the success of ML approaches mostly comes from the quantity of labeled
data used for training. Unfortunately, in most cases the vast majority of
data is unlabeled, and labelling is a costly and time consuming process. 
As opposed to big corporations like \emph{Google} or \emph{facebook}, that can
easily afford to allocate the resources required to label large amounts of data, 
small to medium-sized businesses can only do so on a much smaller scale. 

Since nowadays machine learning based text categorization is widely spread
among many areas of industry, being able to analyze and categorize natural
language finds many applications and has become a very valuable skill. 
Therefore, it is this work's purpose to explore the possibility to achieve high
quality text classification, despite the scarcity of labeled data.

\section{Problem Statement}

As mentioned before, the effectiveness of machine learning based text
classification is mainly data-driven. It is noteworthy that most machine
learning algorithms used in TC, have no notion of the semantics behind the words
contained in documents they classify: a word is usually represented as a dimension
in a vector space with hunders of thousands of other dimensions. E.g in
probabilistic classifiers, such as Naive Bayes, it is the word statistics
generated by the large number of preclassified documents that provide the information relevant to the
underlying classification task.
As a consequence, classifiers trained with too few data samples, lack of
robust statistical information, which directly impacts the classification
effectiveness. Also, due to the high complexity of natural language,
unknown documents are very likely to contain words not covered by the
classification model --- a.k.a. \emph{out of vocabulary (OOV)} words.

By contrast, the way how humans categorize text is heavily based on the
knowledge about semantic relationships between words. It is this internal representation
of language that allows humans to classify text without first reading thousands of example documents. 
With this in mind, we could ask the question:  Is it possible to extend ML
based TC by an \emph{exogenous} knowledge source about word semantics in order
to compensate the lack of the labeled samples?

\section{Suggested Solution}

\subsection{Word Embeddings}
In recent years, Milokov et al. proposed a method to represent words as vectors
in a continuous low dimensional vector space, so
called \emph{word embeddings} \textbf{[citation needed]}.
A particular property of this approach is that the angle between two word
vectors is a measure of how dissimilar the respective words are from a
semantical point of view.
Moreover, it is possible to answer analogy questions of the form ``\emph{woman} is to
 \emph{queen} as \emph{man} is to?'' by simply computing $\mathit{queen} - \mathit{woman} +
\mathit{man} \approx \mathit{king}$ in the embedding vector space. 

We propose to use the semantic information encoded in word embeddings and the
induced distance metric to improve classification accuracy inspite of the scarcity of labeled
data. The following example gives a coarse idea of the intended solution.

\subsection{Example}
Assume we want to classify movie reviews, based on whether they
express approval or disapproval towards the film in question. For the sake of
simplicity let's assume that our training set contains only four samples, two
for each class

\begin{itemize}
  \item Positive samples
  \begin{itemize}
	  \item \emph{A good movie.}
	  \item \emph{Just excellent.}
  \end{itemize}
  
  \item Negative samples
  \begin{itemize}
	  \item \emph{As predictable as could be.}
	  \item \emph{An outstanding example of boring.} 
  \end{itemize}
\end{itemize}

The adjectives used in the dataset are distributed as follows over the classes

\begin{center}
\begin{tabular}{l|l|l}
\textbf{Word} & \textbf{Positive Class} & \textbf{Negative Class}  \\
\hline
boring & 0 & 1 \\
excellent & 1 & 0 \\
good & 1 & 0 \\
outstanding & 0 & 1 \\
predictable & 0 & 1 \\

\end{tabular}
\end{center}

The statistics of the dataset might suggest that the word ``outstanding'' is an
indicator for a negative review. However, since it is used in a rather ironic
way, the document containing it might represent an outlier in its class. A
simple approach to overcome this problem, is to use the word similarity
in the word vector space to merge words with high semantic similarity, e.g. by
computing the pairwise word distances in the emebedded word vector space.
For this example one could perform following substitutions

\begin{center}
\begin{tabular}{l|l|l}
\textbf{Word} & \textbf{Merge with}  \\
\hline
boring & predictable \\
excellent & good \\
outstanding & good \\
\end{tabular}
\end{center}

The new word statistics change as follows

\begin{center}
\begin{tabular}{l|l|l}
\textbf{Word} & \textbf{Positive Class} & \textbf{Negative Class}  \\
\hline
boring & 0 & 2 \\
good & 2 & 1 \\
\end{tabular}
\end{center}

Since the word ``outstanding'' has been replaced with ``good'', its
statistical bias towards the negative class has been smoothed away by the
occurences in the positive class of its synonyms ``good'' and ``excellent''.
 
In the same way, OOV words could be mapped to words known by the
classifier. Assume the task is to predict the class of the sentence ``Fantastic movie''. 
Using the distance metric provided by word embeddings one could substitute the
OOV word ``Fantastic'' with ``excellent'' or ``good'' and thus provide the
classifier with important information for the underlying classification task.

\section{Contributions}

As shown in the previous example, it is possible to leverage knowledge
about word semantics to reduce the statistical bias of unfrequent words and to
offer a possibility to find alternatives in case of OOV words.
We claim that due to these effects, classification accuracies can be
improved by preprocessing the dataset with substitutions of semantically similar words.

For this purpose, we propose a novel \emph{word distance metric} that combines
both \emph{distributional} and \emph{semantic information} about words contained
in the training set. The suggested metric can be used in connection with state
of the art clustering algorithms to preprocess textual data prior to classification.
Especially when training datasets are small, the improvement should be all the more
evident. Figure \ref{fig:hyptohesis} qualitatively shows the predicted
classification accuracies with datasets preprocessed with our method compared
to unpreprocessed datasets. Moreover, this would also open the possibility to use features with higher
information content, such as \emph{$N$-Grams} and \emph{PoS-Tags} (see chapter
\ref{ch:problem-analysis}), even when it would normally result in highly sparse
feature vectors.

\textbf{[Hier kommt noch eine kurze Abhebung von Ã¤hnlichen, bekannten Methoden mit
entsprechenden Referenzen.]}


\section{Scope and Structure}
The effectiveness of the proposed method, will be evaluated experimentally using 
state of the art classifiers such as Naive Bayes and SVM. The datasets used for
evaluation, are both German datasets, provided by \emph{Echobot GmbH} 
(see section \ref{sec:echobot}), and publicly available English datasets, 
that have been used for benchmark in numerous previous research. 


In Chapter \ref{ch:problem-analysis} the problems our solution intends
to solve are analyzed in more detail by first introducing a real world
scenario, in which a similar solution approach is successfully applied by a
human agent (section \ref{sec:echobot}). In section \ref{sec:formalization} text
classification is introduced formally, whilst establishing a framework of
terms and notations that will be used in the course of this work. Moreover, the
statistical bias of unfrequent words and OOV words, will be formally described 
and analyzed in section \ref{sec:statistics}. To be continued \ldots
 