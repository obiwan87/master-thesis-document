%% LaTeX2e class for student theses
%% sections/abstract_en.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3, 2016-12-29

\Abstract
\emph{Text Classification} (TC) is the task of automatically assigning textual data to a set of predefined classes.
Besides being an intensively researched topic, TC and its applications are widely spread among every area of industry.
Nowadays, the underlying algorithms for TC are mainly founded on \emph{machine learning} techniques.  
Typically, the predictive power of machine learning based classifiers is heavily dependent on the quantity and quality of data 
used for training. 
However, this requires gathering labeled data, which is a time- and resource-intensive process that has to be manually performed 
by human domain experts.
In many real-word scenarios, this fact represents a significant limitation to TC effectiveness. 
Therefore, in this work we propose a method to improve text classification accuracy despite the scarcity of pre-classified textual data. 
For this purpose, we contribute a novel \emph{semantic-distributional word distance measure} that can be used in connection with clustering algorithms, 
to extract a semantically enriched and lower dimensional term feature space. Our distance measure is theoretically founded on
\emph{Bayesian Hypothesis Testing} and combines semantic information provided by word embeddings \cite{mikolov2013distributed} 
with distributional information tied to the specific underlying classification task. We evaluate our method with state-of-the-art 
classifiers such as Multinomial Naive Bayes and Support Vector Machines. As opposed to previous 
attempts of term clustering \cite{baker1998distributional, ma2015using}, our method achieves improvements 
in text classification accuracy in all the datasets evaluated. 