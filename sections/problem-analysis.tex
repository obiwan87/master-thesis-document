\chapter{Problem Analysis}
\label{ch:problem-analysis}
% 
% \section{Snippet 1: Advantage ML}
% The main advantage of using ML techniques lies in the fact that classification
% rules can be generated by an \textit{automated} and \emph{generalizing} inductive learning process. 
% Not only classification accuracies of ML approaches are comparable with those of
% human experts, but since the process is performed by computers, it allows for savings
% in terms of labor power and makes the transferral to different application
% domains easier.\footnote{From now on, if not specified otherwise, whenever this work
% refers to \textit{text categorization}, \textit{text classification} or \textit{TC}, it
% will be with respect to ML based methods.}
% 
%  Nonetheless, due to the high variability and complexity of human
% language, automatic text classification is presented with many non-trivial challenges 
% that are yet to be mastered and worthwhile to be further researched. 
% 
% \section{Snippet 2: KE}
% 
% The importance of categorizing and organizing text data already
% arose scientific interest in the 60's of the last century. However, it was only
% in the 1980's that first implementations were used in productive environments. 
% In its early versions TC required a \textit{domain expert} to define a set of
% rules with which documents could be assigned to their corresponding class or set
% of classes. This is also known as \textit{knowledge engineering} (KE).
% However, with the arrival of more powerful hardware and with the growth of
% information available in digital format, the aforementioned approach lost
% popularity and was mainly replaced by \textit{machine learning} (ML)
% based text classification.

\section{Case Study: Echobot}
\label{sec:echobot}
An example of the importance and at the same time of the difficulties of text
classification can be found at \textit{Echobot}, a small
German company that develops machine learning based tools for social media
monitoring and sales. Their goal is to offer their customers solutions that
help oversee what the web is reporting about their products, their firm or
their competitors, helping them to adjust their business strategy according 
to trends detected on the web. Web content is retrieved by a complex web crawler
that scans millions of social media postings and hundreds of thousands
of articles and company websites daily. The information gathered in this way is
analyzed, structured and categorized with machine learning based software that
extracts useful and relevant information for the customer.

In one specific context, text classification is used to alert customers when
newly published web content concerns a specific topic. E.g. one might
be interested in knowing when a company of a specific industry is about to change
their management personell or has announced a site closure. 
Events like this, also called \textit{business signals}, are of particular
commercial interest for the customer. It is e.g. known that a restructuring 
of a company's managing personell, is often followed by the purchase of new
services and products. With this knowledge at hand, a salesman that has been
alerted of such event, can contact and make an offer before anyone else.

Currently Echobot's system supports the recognition of around a dozen of 
different business signals, yet due to its usefulness for the customer, the
demand for developing new business signals is growing. 
Echobot faces the same problems previously mentioned: labeled data is scarce and
costly to produce, but nonetheless crucial for effective text categorization.
Moreover, since Echobot is a small company, it is not viable to allocate the 
resources needed to label large amounts of data. Therefore, in order to optimize
classification accuracy inspite of the scarcity of labeled data, Echobot developed 
a hybrid process combining ideas of KE and ML. Instead of building a
classification model based on the raw textual data, a domain expert
preprocesses the documents by annotating each sample with semantic and
syntactic information that will help the classifier build a more precise
classification model.

Why is it necessary to heavily involve a human agent in the classification
process? Given the complexity of natural language, state of the art
classifiers trained with too few labeled documents might not be able to build a
generalizable and accurate classification model. This can also be observed
experimentally. Compared with its unpreprocessed variant, classification
with the annotated dataset performs significantly better.

Generally speaking, the described hybrid method leverages the fact that text
classification performance is improved by introducing external knowledge
about textual syntax and semantics. In this case, it is provided by a domain expert.
He defines specific features that will increase the classifier's confidence when
predicting a sample's class.
From a more abstract view, a question arises about the whole classification
approach: Would it be possible to \textit{automatically} --- i.e. without human
agents --- introduce external knowledge about language semantics in order to
improve the classification performance?  

\section{Formalization of Text Classification}
 
 \label{sec:formalization}
For the sake of clarity it is useful to establish a framework of terminology and
notations that will be used in later explanations and in the course of 
the entire master thesis. The notations introduced in this section can be
overviewed in table \ref{tab:notations}.



\subsection{Documents and Vocabulary}

Let $D=\{d_1, \ldots, d_{|D|}\}$ be a \textit{dataset} of $|D|$
documents. Each document $d_i$ is composed of words $V = \{v_1, \ldots,
v_{|V|}\}$, the \textit{vocabulary} of dataset $D$.
The meanings of the terms ``word'' and ``document'' don't necessarily coincide
with their colloquial counterpart. A ``word'' can also be a sequence of numbers
and/or symbols and a ``document'' is simply an ordered collection of ``words''.
As a matter of fact, the segmentation of a ``document'' into ``words'' is done
during a process called \emph{tokenization}, as a result of which a
``word'' is also refered to as \emph{token}.

More formally, a document $d_i$ is a tuple with $|d_i|$ elements such that
$d_i = (w_1^i,\ldots,w_{|d_i|}^i)$ with $w_j^i \in V$.
Therefore, whenever this work refers to ``words in a document'', instead of
meaning ``word'' in the sense of a lexically valid word, we mean the elements
$w_j^i$ of a $|d_i|$-tuple $d_i$.

\subsection{Classification}

A document's $d$ true class is written as $\omega_{d}$ ---  a.k.a.
\emph{label} of document $d$. It is important to note that each document has exactly one and
only one class associated to it. This induces a partition of the dataset $D$ as follows:

\begin{eqnarray*}
 D = \bigcup\limits_{\omega \in \Omega} D_\omega \\
 D_\omega \coloneqq  \{ d \mid \omega_d = \omega\} \\
\forall \omega' \neq \omega'' \in \Omega: D_{\omega'} \cap D_{\omega''} =
 \emptyset 
\end{eqnarray*}

In the scope of this work, we will only consider the most general case of
\emph{binary classification}, i.e. a document's label is either ``$+$'' (positive) or
``$-$'' (negative), i.e.  $\Omega = \{-,+\}$. 
 
 \subsection{Word Counts}
 
Let $N_k^i$ be the count of occurences of word $v_k$ in document $i$. Also, let
$N_{k,\omega}$ refer to the number of documents with label $\omega_d = \omega$ 
that contain word $v_k \in V$.

Another way to express this is
\begin{eqnarray*}
 N_k^i \coloneqq \sum\limits_{j=1}^{|d_i|} \mathbb{1}\{w_j^i = v_k\} \\
 N_{k,\omega} \coloneqq \sum\limits_{d \in D_\omega} \mathbb{1}\{N^i_k \geq 1\} 
\end{eqnarray*}
where $\mathbb{1}\{\cdot\}$ denotes the characteristic function.

From this, a definition for the word counts over all labels $\Omega$ follows
directly:

\begin{eqnarray*}
N_k \coloneqq \sum_{\omega'\in \Omega} N_{k,\omega'} 
\end{eqnarray*}

\subsection{Bag Of Words}

Usually a classifier does not perform a direct mapping from
the raw textual data to a predicted class $\hat{\omega}$. Instead, prior to the actual
classification, documents $d \in D$ are mapped to a numerical vector
space $\mathcal{F}$, also called \emph{feature space}, on the base of which an internal 
classification model and finally the classification function are computed.

A widely spread state of the art approach in text classification to represent
documents as numerical vectors is the \emph{Bag-of-Words (BoW)}.
A document $d$ can be represented as a BoW in the following way

\begin{eqnarray*}
	\mathcal{B} := \mathbb{N}^{|V|} \\
	\mathbf{b}_i \in \mathcal{B} \\
	\mathbf{b}_i \coloneqq (b_1^i, b_2^i, \ldots, b_{|V|}^i)^\intercal \\
	b_k^i \coloneqq N_k^i
\end{eqnarray*}
	
In other words, a BoW is a vector of positive integers and zero, whose
dimensionality equals the size of the vocabulary $V$. The value $b_k^i$
indicates how many occurences of word $v_k$ there are in document
$d_i$. 

For the sake of brevity and readability we also introduce the following
notations:

\begin{eqnarray*}
	\mathbf{b}_i: V \mapsto \mathbb{N} \\
	\mathbf{b}_i(v_k) := b_k^i,\ v_k \in V
\end{eqnarray*}

Since by definition the objects $\mathbf{b}_i: V \mapsto \mathbb{N}$ and
$\mathbf{b}_i \in \mathcal{B}$ are structurally identical, they will be used interchangeably, 
depending on which notation is best suited for an argument. 


[Add Training/Validation/Evaluation split?]
[Classification function c directly as function of feature space?]

\begin{table}
\begin{tabular}{|l|l|l|}

\textbf{Notation} & \textbf{Description} & \textbf{Example} \\

$D$ & set of preclassified documents & $D=$\{('Hello', 'world'),
('Hello') \} \\

$D_\omega$ & set of documents of class $\omega$ & $D_+ = \left\{
\text{('Hello', 'world')} \right\}$ \\
& & $D_- = \left\{ \text{('Hello')} \right\}$ \\ 

$V$ & vocabulary of $D$ & $V=$\{'Hello', 'world'\} \\

$N_k^i$ & counts of word $v_k$ in document $d_i$ & $N_2^1 = 1,\ N_2^2 = 0$ 
\\

$N_k$ & number of documents containing word $v_k$ & $N_2 = 2$ \\

$N_{k,\omega}$ & counts of $v_k$ in $D_\omega$ & $N_{2, -} = 0$ \\ 

$\mathbf{b}_i$ & BoW of document $d_i$ & $\mathbf{b}_1 = (1, 1)^\intercal,\
\mathbf{b}_2 = (1,0)^\intercal$ \\

$\mathbf{b}(v_k)$ & Value of $v_k$ in BoW $\mathbf{b}$ &
$\mathbf{b_2}(\text{\lq world\rq}) = 0$
 
\end{tabular}
\caption{Formalization of \emph{TC}: Overview of Notations}
\label{tab:notations}
\end{table}

 \section{Higher Level Features}
 \label{sec:statistics}
In BoW representations, the order in which words occur in a document is
neglected. Inspite of this, representing documents as BoWs has proven
to work reasonably well for many text classification tasks. A possible reason why this approach works, is
that the statistics of word occurences provide a good measure for predicting a
document's class. E.g. the Naive Bayes classifier estimates the posterior
probability $P(\omega|d)$ of a class $\omega$ given a document $d$ by
essentially counting the occurences of each word for each class.
\textbf{[Use optimal Bayes classifier instead?]}

However, the order of in which words occur can often yield important information
that are relevent for the classification tasks. Consider the sentences ``no,
it's good" and ``it's no good". Despite expressing the opposite of each other,
in their BoW representation with the vocabulary $V = \left\{\text{'no', 'it's', 'good'} \right\}$ 
the two sentences would be identical. 

\subsection{N-Grams}

A common technique that is used in order to represent the local order
of words is to use \emph{$N$-grams}. In text classification, an $N$-gram is a
contiguous sequence of $N$ words from a given document. $N$-grams of size one are called
\emph{unigrams}, of size two \emph{bigrams}, and of size three \emph{trigrams}.
$N$-grams for $N > 3$ are referred to as four-grams, five-grams, six-grams, etc. 

E.g. consider the following sentence:

\begin{center} 
\textit{It's a beautiful day}
\end{center} 

Here the unigrams are

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
It's & a & beautiful & day \\
\hline
\end{tabular}
\end{center}

The bigrams are 

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
<s> It's & It's a & a beautiful & beautiful day & day </s> \\
\hline
\end{tabular}
\end{center}

where ``<s>'' and ``</s>'' are tokens that denote the start and the end of the
sentence respectively. A common approach to include $N$-grams in a dataset is to
concatenate each document with its $N$-grams as follows: 

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
It's & a & beautiful & day & <s> It's & It's a & a beautiful & beautiful day &
 
day </s> \\
\hline
\end{tabular}
\end{center}

A representation as BoW follows directly by representing the bigrams as
tuples of contiguous unigrams $(w_k^i, w_{k+1}^i)$. 

 
\subsection{Part-Of-Speech Tagging}
So far we have seen that using $N$-grams allows to add sequential information to a Bag-Of-Words representation of a document. 
As matter of fact, it is not the only method used to enrich a Bag-Of-Words
vector with additional information. Another approach, is to so called
\emph{Part-Of-Speech (PoS) tagging}. This refers to annotating each word with
its grammatical function within the sentence it occurs. For our example sentence this would
 look like

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Word} & \textbf{PoS} \\
\hline 
It 		  & \emph{pronoun - predicative subject of ``is''} \\
's 		  & \emph{finite verb - predicate} \\ 
a 		  & \emph{indefinite article of ``day''}  \\
beautiful & \emph{adjective of ``day''} \\
day		  & \emph{noun - subject of ``is''} \\
\hline
\end{tabular}
\end{center}

In the same way as with $N$-grams, the syntactic information provided by PoS-tags
can, given enough samples, improve classification accuracy \textbf{[citation
needed]}.
Nonetheless, the statistical bias due to the high number of different of
word-PoS pairs combinations remains.

As seen so far, appending additional information $N$-grams or PoS-tags can be used as a method
to enrich BoW-vectors with syntactic information, yet cannot be
expected to generally improve classification accuracy due to the induced
statistical bias. Note that the number of PoS-Tags grows asymptotically 
in $\mathcal{O}(|Unigrams|^2)$ while the number of N-Grams grows in
$\mathcal{O}(|Unigrams|^N)$, which not only makes the the feature statistics
heavily sparse, but also doesn't scale well in terms of computation time and memory consumption.

\subsection{Sparse Feature Vectors and Sampling Error}


In \textbf{[citation needed] X et. al} experimentally showed that just by adding
bigrams as described in the previous example, classification accuracies can be
slightly improved.
Nonetheless, using \textit{trigrams} didn't make the classification accuracy better. 
One way to explain why this happens, is by considering how the
statistics of each $N$-gram are affected, when $N$ grows.
 
Assume a unigram $v$ appears $N_v$ times in a given dataset $D$. All bigrams
$(v,v')$ for $v' \neq v$ can't possibly occur more often than  $v$ itself.
Generally speaking $N_{(v,v')} \leq N_v$. 
The same logic can be applied to $N$ and $N+1$ for $N >= 2$.\footnote{W.l.o.g.
this can also be said about bigrams $(v'',v)$ with $v '' \neq v$.} In the
extreme case, when $N = \max_{d' \in D} |d'|$, then every $N$-gram occurs at most one time. In short,
what we observe is that by building bigger and bigger $N$-grams we reduce the 
average count of $N$-grams per class. From a statistical point of view this
might present a problem. In its classical interpretation, the probability of an event, 
in this case that an 
$N$-gram $v_k$ appears in class $\omega$, is approximated by
its relative frequency with respect to the counts of $v_k$ in the whole dataset: 

\begin{equation}
	P(w|\omega) \approx \frac{N_{k,\omega}}{N_k} =:
	\hat{P}(w|\omega)
\end{equation}

The main claim of frequentist probability is that for inifnite samples the
relative frequency converges to the true underlying probability, in the same 
way as the relative frequency of throwing ``heads'' with a fair coin, will
approach 0.5 with a growing number of coin throws. Therefore, as we consider the
number of observed documents $|D|$ approaching infinity, we obtain 

\begin{equation}
	P(w|\omega) = \lim_{|D| \to \infty} \frac{N_{k,\omega}}{N_k}
\end{equation}

With fewer occurences for each $N$-gram, the estimates for the word probabilities
become more biased due to the scarcity of observations. This phenomenon is also
known as \emph{sampling error}. \footnote{As a matter of fact, if we assume a
gaussian distribution, the sampling error $\epsilon \in
\mathcal{O}(1/\sqrt{|D|})$} This error propagates to the estimated posterior probability $\hat{P}(\omega|d)$. 
The result is that words with low counts act like noise, which on average has no
impact on classification accuracy at all or can even worsen a classifier's
predictive ability.

\section{Solution}
One way to avoid the introduction of noisy features and to reduce the
dimensionality of the feature space, is to precede the classification with a
\emph{feature selection} and \emph{transformation} procedure. In case of
business signal classification, described in the previous section, this is
exactly the task of the human agent. In this case the linguistic skills of the 
domain expert are leveraged for the selection and combination of PoS tags
and $N$-grams. Instead of arbitrarily adding $N$-grams and PoS-tags, that
could potentiall harm classification quality, the operator chooses carefully a
subset of features that is meaningful for the underlying classification task. 
Moreover, as opposed to a standard feature selection task, the operator reduces
the dimensionality of the feature space by creating rules to combine different 
features into new ones.

The operations of the domain expert can be described mathematically as follows: 
let $D$ be a set of documents containing unigrams $V_U$, $N$-grams $V_N$ and
PoS-Tags $V_P$ such that $V = V_U \cup V_N \cup V_P$. Based on this vocabulary, the resulting BoW
feature vector has dimension $|V| = |V_U| + |V_P| + |V_N|$.
The operator's task is to combine the available unigrams, $N$-grams and PoS-Tags
such that the resulting feature space reflects his understanding
of semantics for the underlying classification task.
More formally, this implies the creation of a list of rules
$\boldsymbol{\rho} = (\rho_1, \rho_2, \ldots, \rho_n)$, where each rule $\rho_i:
\mathcal{B} \to \mathbb{N}$ for $i=1,2,3,\ldots,n$ maps a subspace of the BoW
feature space $\mathcal{B}$ to a scalar value.
[The operations performed by the rules $\rho_i$ of logical, arithmetic or
semantic nature.]

\begin{eqnarray*}
\boldsymbol\rho: \mathcal{B} \to \mathcal{H} \\
\boldsymbol{\rho}(\mathbf{b}) = (\rho_1(\mathbf{b}), \ldots,
\rho_n(\mathbf{b}))^\intercal \\
\mathcal{H} \coloneqq \mathbb{N}^n
\end{eqnarray*}

In other words, after applying $\boldsymbol\rho$, the classification problem is
transfered from the large feature space of BoW's to a much lower dimensional and semantically optimized
vector space engineered by the domain expert.

The following toy example illustrates how such a rule might look
like for a specific vocabulary: 

\begin{eqnarray*}
V_U \supset \{\text{'good', 'not', bad'}\}, V_P \supset \{\text{``not'' - particle -
negates\ ``bad"'}\} \\
\rho_1(\mathbf{b}) = \mathbf{b}(\text{'good'}) \lor \left[\mathbf{b}(\text{'bad'})
\land \mathbf{b}(\text{'``not'' - particle - negates\ ``bad"'}\right]
\end{eqnarray*}

The rule above merges the words ``good'' and ``not bad'' to one new feature.
This is done by checking whether the word ``bad'' appears in combination with a
``not"-particle that negates it. Note that the same result could be achieved
by using the bigram ``not bad'' and merging it with the unigram ``good''.
In a task of \emph{sentiment analysis} this would be a sensible rule to
distinguish whether the word ``bad'' was used in a negative or in a positive
way. This cannot be achieved if the words ``not''
and ``bad'' are represented separately as unigrams, as the word ``not'' could a
appear in a context that does not refer to ``bad''.
Also, since the occurences of the term ``good'' and ``not bad'' are treated
equally, their statistics will be combined and hence will be more robust towards
sampling errors.

As seen from the previous example, the engineered feature space levigates the
problems described in this section, namely the decrease in statistical quality of
features and the lack of ordering information. Experiments show that a
classification model trained with a dataset preprocessed in this way, achieves significantly 
better classification accuracies than a Bag-Of-Words composed of unigrams and/or
bigrams.

\subsection{Out of Vocabulary Words}
\label{sec:problem-statement}

Another challenge in text classification is \emph{Out-Of-Vocabulary (OOV)} 
words. OOV words are encountered in the case in which an unknown document, i.e.
a document for which the true label is not known,
contains words that are not part of the classifier's known vocabulary. 
As a consequence, the information provided by OOV words cannot be used for
prediction, since they do not have any representation in the classifier's feature 
space. 

One possible way to tackle this problem is to introduce external knowledge about
\textit{synonymie}, thus allowing a mapping to known words without distorting
the original meaning.
In the context of echobot's business signal classification, this problem is again 
solved by exploiting the domain expert's linguistic and taxonomic understanding
of words. He preemptively creates a set of synonyms to which the classifier can
fall back to in case a word is not contained in the known vocabulary.

We can define this operation as a function $\tau: \tilde{V} \to V$, that
maps elements of the set of OOV words $\tilde{V}$ to the set of known
vocabulary $V$. Here the set $T_v = \left\{ \tilde{v} \in \tilde{V} \mid
\tau(\tilde{v})=v \right\} $ can be interpreted as a set of synonyms or lexical
substitutes for the word $v$.


\subsection{Summary}

\textcolor{red}{\textbf{Ab hier noch nicht Ã¼berarbeitet}}

The two problems identified so far, namely the decrease in
statistical quality and OOV words, can be alleviated by involving a human agent
with linguistic knowledge.
In this case the human agent creates a collection of rules $\boldsymbol\rho:
\mathcal{B} \to \mathcal{H}$ that maps the set of available features to an engineered
feature space $\mathcal{H}$ that will --- with best knwoledge of the domain
expert --- increase the expected classification accuracy. In case of OOV words
an operation $\tau: \tilde{V} \to V$ is performed that substitutes unknown words
that, that could have discriminating quality for the underlying classification, to words known by
the classifier. 

The goal of this master thesis is to find an operation
$\boldsymbol\sigma: \mathcal{B} \to \mathcal{A}$ that approximates the
behaviour of the human agent with respect to the problems reported earlier and
that is not based on the involvement of a human domain expert. In other words,
we will answer the question of whether it is possible to introduce an external
knowledge source to the classification process that helps to include features
with higher information content whilst reducing the induced sampling
error and make OOV words useful to the classification process.

\section{Suggested Solution}

The semantic knowledge about language allows a human expert to generate a
feature space, such that the information contained in the training dataset is
maximally exploited in order to achieve higher classification accuracies. It is
this semantic understanding, that allows human operator to merge $N$-grams and
PoS-annotated words with others in order to minimize the sampling error.

Moreover, OOV words could be replaced with similar words known by the
classifier.
A metric that operates in this way, can be designed by using \textit{word
vectors}. In recent years, Milokow et. al \textbf{[citation needed]} developed a method
to embed words in vector space, which involves shallow learning with neural
networks --- for obvious reasons this technique is also called \textit{word embeddings}. His
implementation has become known as \emph{word2vec}, which is short for
word-to-vector. Using their  vectorial representation, it is then possible to
measure the similarity between words by simply computing the angle between the
corresponding vectors. This method has been exhaustivley benchmarked and tested
on tasks of lexical substitution, word sense disambiguation, sentiment
analysis, etc.

Let $V$ be the vocabulary of a dataset and $\tilde{V}$ the set of OOV words. The
aforementioned preprocessors can then be described as two functions $\sigma_1:
\tilde{V} \to V$ and  $\sigma_2: V \to V'$ where $V' \subsetneq V$ and
$\tilde{V} \cap V = \emptyset$. [The entire process can be formulated as $\sigma
\equiv \sigma_1 \circ \sigma_2$]. [Figure needed]




