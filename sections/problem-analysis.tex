\chapter{Formalization and Problem Analysis}
\label{ch:problem-analysis}

\section{Theoretical Foundation of Text Classification}
 
 \label{sec:formalization}
For the sake of clarity it is useful to establish a framework of terminology and
notations that will be used in later explanations and in the course of 
the entire master thesis. The notations introduced in this section can be
overviewed in table \ref{tab:notations}.

\subsection{Documents and Vocabulary}

Let $D=\{d_1, \ldots, d_{|D|}\}$ be a \textit{dataset} of $|D|$
documents. Each document $d_i$ is composed of words $V = \{v_1, \ldots,
v_{|V|}\}$, the \textit{vocabulary} of dataset $D$.
The meanings of the terms ``word'' and ``document'' don't necessarily coincide
with their colloquial counterpart. A ``word'' can also be a sequence of numbers
and/or symbols and a ``document'' is simply an ordered collection of ``words''.
As a matter of fact, the segmentation of a ``document'' into ``words'' is done
during a process called \emph{tokenization}, as a result of which a
``word'' is also refered to as \emph{token}.

Formally a document $d_i$ is a tuple with $|d_i|$ elements such that
$d_i = (w_1^i,\ldots,w_{|d_i|}^i)$ with $w_j^i \in V$.
Therefore, whenever this work refers to ``words in a document'', instead of
meaning ``word'' in the sense of a lexically valid word, we mean the elements
$w_j^i \in V$ of a $|d_i|$-tuple $d_i$.

We also introduce the \emph{domain of documents} $\mathcal{D}$ and the
\emph{domain of words} $\mathcal{V}$, that define the infinite sets of all possible
documents and all possible words respecitively, i.e. $D \subset \mathcal{D}$ and $V \subset
\mathcal{V}$.
\subsection{Labels}

Let $\Omega=\left\{\omega_1, \omega_2,\ldots,\omega_{|\Omega|}\right\}$ be the
set of classes defined for the current classification problem. A document's $d \in D$ true class or \emph{label} is written as
$\omega_{d} \in \Omega$.
  
It is important to note that each document has exactly one and
only one class associated to it. This induces a partition of the dataset $D$ as follows:

\begin{equation*}
\begin{split}
 D &= \bigcup\limits_{\omega \in \Omega} D_\omega \\
 D_\omega &\coloneqq  \{ d \mid \omega_d = \omega\}, \omega \in \Omega \\
\forall \omega' \neq \omega'' \in \Omega &: D_{\omega'} \cap D_{\omega''} =
 \emptyset
 \end{split} 
\end{equation*}

In the scope of this work, we will only consider the most general case of
\emph{binary classification}, i.e. a document's label is either ``$+$'' (positive) or
``$-$'' (negative), i.e.  $\Omega = \{-,+\}$. 
 
 \emph{Unknown documents} are all documents $\tilde{d} \in \mathcal{D}\setminus
 D$ for which the label is not known.
 
 \subsection{Word Counts}
 
Let $N_k^i$ be the count of occurences of word $v_k$ in document $i$, i.e.

\begin{equation*}
 N_k^i \coloneqq \sum\limits_{j=1}^{|d_i|} \mathbb{1}\{w_j^i = v_k\} \\ 
\end{equation*}

where $\mathbb{1}\{\cdot\}$ denotes the characteristic function.

Also, let $N_{k,\omega}$ refer to the number of documents with label $\omega_d = \omega$ 
that contain word $v_k \in V$:

\begin{equation*}
  N_{k,\omega} \coloneqq \sum\limits_{d \in D_\omega} \mathbb{1}\{N^i_k \geq 1\}
\end{equation*}

A definition for the word counts over all labels $\Omega$ follows
directly:

\begin{eqnarray*}
N_k \coloneqq \sum_{\omega'\in \Omega} N_{k,\omega'} 
\end{eqnarray*}

\subsection{Bag Of Words}

Usually a classifier does not perform a direct mapping from
the raw textual data to a predicted class $\hat{\omega}$. Instead, prior to the actual
classification, documents $d \in D$ are mapped to a numerical vector
space, also called \emph{feature space}, on the base of which an internal 
classification model and finally the classification function are computed.

A widely spread state of the art approach in text classification to represent
documents as numerical vectors is the \emph{Bag-of-Words (BoW)}.
A document $d$ can be represented as a BoW in the following way

\begin{eqnarray*}
	\mathcal{B}_V := \mathbb{N}^{|V|} \\
	\mathbf{b}_i \in \mathcal{B}_V \\
	\mathbf{b}_i \coloneqq (b_1^i, b_2^i, \ldots, b_{|V|}^i)^\intercal \\
	b_k^i \coloneqq N_k^i
\end{eqnarray*}
	
In other words, a BoW is a vector of positive integers and zero, whose
dimensionality equals the size of the vocabulary $V$. The value $b_k^i$
indicates how many occurences of word $v_k$ there are in document
$d_i$. 

For the sake of brevity and readability we also introduce the following
notations:

\begin{eqnarray*}
	\mathbf{b}_i: V \mapsto \mathbb{N} \\
	\mathbf{b}_i(v_k) := b_k^i,\ v_k \in V \\
	\mathbf{b}_V: \mathcal{D} \to \mathcal{B}_V \\
	\mathbf{b}_V(d) = N_V(v,d) 
\end{eqnarray*}

Since by definition the objects $\mathbf{b}_i: V \mapsto \mathbb{N}$ and
$\mathbf{b}_i \in \mathcal{B}_V$ are structurally identical, they will be used
interchangeably, depending on which notation is best suited for an argument. 

\subsection{Word Embeddings}

Word embeddings are a more recent approach to represent words as numerical
vectors. In \cite{mikolov2013distributed} Milokov et al. proposed to generate
word vectors with a shallow learning network that estimates the
probability $\Pr(w_O|w_I)$ of an output word $w_O$ given their context $w_I$.
The network is usually trained in an unsupervised fashion by using a large
text corpus, e.g. the first billion words of \emph{Wikipedia}. 

For a corpus with vocabulary $V_\mathcal{W}=\left\{ v^1_\mathcal{W}, \ldots,
v^{|V_\mathcal{W}|}_\mathcal{W}\right\}$ the result of training is a collection
of vectors $\mathbf{x}_1, \ldots,\mathbf{x}_{|V_\mathcal{W}|} \in \mathbb{R}^d$, 
each of which represents a word $v \in V_\mathcal{W}$ in a
$d$-dimensional real-valued vector space. Note that the dimensionality of the
vector space is chosen as a parameter prior to the training process. An often
used and empirically well working dimensionality seems to be $d=300$. We
denote a word embedding with vocabulary $V_{\mathcal{W}}$ and 
word vectors $\mathbf{x}_i$ as a tuple $\mathcal{W}=(V_{\mathcal{W}},
(\mathbf{x}_i))$.

We denote the vector representation of a word as follows:

\begin{gather*}
	\mli{vec}: V_\mathcal{W} \to \mathbb{R}^d \\
	\mli{vec}(v^i_{\mathcal{W}}) = \mathbf{x}_i
\end{gather*}

As reported in \cite{mikolov2013distributed} the vector representations
of words can be used to measure the semantic similarity between words. This can
be done by computing the angle between two word vectors as follows:

\begin{gather*}
	\mli{sim}: \mathbb{R}^d \times \mathbb{R}^d \to [-1, 1] \\
	\mli{sim}(\mathbf{x}_1, \mathbf{x}_2) =
	\frac{\mathbf{x}_1^\intercal\mathbf{x}_2}{\norm{\mathbf{x}_1}_2 \cdot
	\norm{\mathbf{x}_2}}_2 = \cos \angle (\mathbf{x}_1, \mathbf{x}_2)
\end{gather*} 
 
This measure is referred to as \emph{cosine similarity}, since it is
equivalent to calculating the cosine of the angle between two vectors. In the
embedding word vector space it, ranges from $-1$ (very unsimilar words) to $1$
(identical words).
A measure for dissimilarity can be expressed by using the \emph{cosine distance}:

\begin{gather*}
	\mli{dist}: \mathbb{R}^d \times \mathbb{R}^d \to [0, 2] \\
	\mli{dist} \equiv 1 - \mli{sim}
\end{gather*}
Note that strictly speaking, the cosine distance is not
a \emph{metric} since it does not satisfy the \emph{triangle
inequality}, i.e.

\begin{equation*}
	\exists \mathbf{x},\mathbf{y},\mathbf{z} \in \mathbb{R}^d: \mli{dist}(\mathbf{x},\mathbf{z})
	\not\leq \mli{dist}(\mathbf{x},\mathbf{y}) + \mli{dist}(\mathbf{y},\mathbf{z})
\end{equation*}

However, whenever this work refers to a \emph{distance} with respect to
words or word vectors, we refer to a measure that expresses their dissimilarity
such as the just introduced cosine distance.

\begin{table}
\begin{tabular}{|l|l|l|}

\textbf{Notation} & \textbf{Description} & \textbf{Example} \\

$\mathcal{D}$ & set of all possible documents & \\
$\mathcal{V}$ & set of all possible words & \\
$D$ & set of preclassified documents & $D=$\{('Hello', 'world'),
('Hello') \} \\

$D_\omega$ & set of documents of class $\omega$ & $D_+ = \left\{
\text{('Hello', 'world')} \right\}$ \\
& & $D_- = \left\{ \text{('Hello')} \right\}$ \\ 

$V$ & vocabulary of $D$ & $V=$\{'Hello', 'world'\} \\
$\tilde{V}$ & OOV words $\mathcal{V} \setminus V$ &  \\
$N_k^i$ & counts of word $v_k$ in document $d_i$ & $N_2^1 = 1,\ N_2^2 = 0$ 
\\

$N_k$ & number of documents containing word $v_k$ & $N_2 = 2$ \\

$N_{k,\omega}$ & counts of $v_k$ in $D_\omega$ & $N_{2, -} = 0$ \\ 

$\mathbf{b}_i$ & BoW of document $d_i$ & $\mathbf{b}_1 = (1, 1)^\intercal,\
\mathbf{b}_2 = (1,0)^\intercal$ \\

$\mathbf{b}(v_k)$ & Value of $v_k$ in BoW $\mathbf{b}$ &
$\mathbf{b_2}(\text{\lq world\rq}) = 0$
 
\end{tabular}
\caption{Formalization of \emph{TC}: Overview of Notations}
\label{tab:notations}
\end{table}

 \subsection{Complex Linguistic Features}
 \label{sec:statistics}
In BoW representations, the order in which words occur in a document is
neglected. Inspite of this, representing documents as BoWs has proven
to work reasonably well for many text classification tasks. A possible reason why this approach works, is
that the statistics of word occurences provide a good measure for predicting a
document's class. E.g. the Naive Bayes classifier estimates the posterior
probability $\Pr(\omega|d)$ of a class $\omega$ given a document $d$ by
essentially counting the occurences of each word for each class
\cite{mccallum1998comparison}.

However, the order in which words occur can often yield important information
that are relevant to the classification task. Consider the sentences ``no,
it's good" and ``it's no good". Despite expressing the opposite of each other,
in their BoW representation with the vocabulary $V = \left\{\text{'no', 'it's', 'good'} \right\}$ 
the two sentences would be identical. 

\subsubsection{N-Grams}
\label{sssec:n-grams}
A technique that is commonly used to represent the local order
of words is to include \emph{$N$-grams}. In text classification, an $N$-gram is
a contiguous sequence of $N$ words from a given document. $N$-grams of size one are called
\emph{unigrams}, of size two \emph{bigrams}, and of size three \emph{trigrams}.
$N$-grams for $N > 3$ are referred to as four-grams, five-grams, six-grams, etc. 

E.g. consider the following sentence:

\begin{center} 
\textit{It's a beautiful day}
\end{center} 

Here the unigrams are

\begin{center}
\begin{tabular}{|c|c|c|c|}
\hline 
It's & a & beautiful & day \\
\hline
\end{tabular}
\end{center}

The bigrams are 

\begin{center}
\begin{tabular}{|c|c|c|c|c|}
\hline 
<s> It's & It's a & a beautiful & beautiful day & day </s> \\
\hline
\end{tabular}
\end{center}

where ``<s>'' and ``</s>'' are tokens that denote the start and the end of the
sentence respectively. A common approach to include $N$-grams in a dataset is to
concatenate each document with its $N$-grams as follows: 

\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|c|c|}
\hline 
It's & a & beautiful & day & <s> It's & It's a & a beautiful & beautiful day &
 
day </s> \\
\hline
\end{tabular}
\end{center}

We define the $N$-grams represantion of a document as follows:

\begin{equation*}
\begin{split}
	g_N &:  \mathcal{D} \to \mathcal{D}, N \geq 2 \\
	d &= (w_1,w_2,\ldots,w_{|d|}) \in \mathcal{D} \\
	g_N(d) &=  (\underbrace{(S_0,w_1, \ldots,
	w_N)}_{\mathrm{Tuple~of}~N\mathrm{~contiguos~words}}, \ldots,
	(w_{|d|-N+2},\ldots,w_{|d|},S_1))
\end{split}
\end{equation*}

where $S_0, S_1 \not\in \mathcal{V}$ are special tokens that define the start
and the end of a document respectively.

The operation of \emph{adding} $N$-Grams to a document is defined as:

\begin{equation*}
\begin{split}
	g_N^* &: \mathcal{D} \to \mathcal{D}, N \geq 2 \\
	g_N^*(d) &= d~\Vert~g_2(d)~\Vert \ldots \Vert~g_N(d)
\end{split}
\end{equation*} 

where $||$ is the tuple-concatenation operator.

A representation as BoW\footnote{A BoW that represents N-Grams is also called a
\emph{Bag-Of-N-Grams}. } follows directly by expanding the vocabulary by tuples of
$2,\ldots,N$ contiguous unigrams and applying $g_N^*$ to all documents in a dataset $D$:

\begin{equation*}
\begin{split}
	V_N &=  \left \{(w_k^i,
	w_{k+1}^i,\ldots,w_{k+N-1}^i) \mid d_i \in D \wedge 1 \leq k \leq |d_i|-N+1
	\right\} \\
	& \cup \{ (S_0,w_1^i,\ldots,w_{k+N}^i)  \mid d_i \in D \} \\
	& \cup \{ (w_{|d|-N+2},\ldots,w_{|d|},S_1) \mid d_i \in D\} \\
	V_N^* &\coloneqq V \cup \bigcup\limits_{n=2}^N  V_n \\
	g_N^*(D) &\coloneqq \left\{g_N^*(d_1),\ldots,g_N^*(d_{|D|})\right\}
\end{split}
\end{equation*}

\subsubsection{Part-Of-Speech Tagging}
Another approach to enrich BoW with additional information about the words
contained in a document, is to so called \emph{Part-Of-Speech (PoS) tagging}.
This refers to annotating each word with its grammatical function within the sentence it occurs. 
For our example sentence this would look like

\begin{center}
\begin{tabular}{|l|l|}
\hline
\textbf{Word} & \textbf{PoS} \\
\hline 
It 		  & \emph{pronoun - predicative subject of ``is''} \\
's 		  & \emph{finite verb - predicate} \\ 
a 		  & \emph{indefinite article of ``day''}  \\
beautiful & \emph{adjective of ``day''} \\
day		  & \emph{noun - subject of ``is''} \\
\hline
\end{tabular}
\end{center}

Generally speaking, PoS-Tagging implies affixing linguistic and
grammatical modifiers to tokens within a document: 

\begin{equation*}
(v,m_1,m_2,\ldots,m_l) \in
V \times M_1 \times M_2 \times \ldots M_l
\end{equation*}
Which modifiers $M_j$ are used, depends on the implementation of the
PoS-Tagger and the document language. Possible modifiers include the word
type (noun, verb, adjective) to the morphological form (grammatical case,
grammatical gender, grammatical number) and the grammatical function 
(subject, object, predicate) of the tagged word.
 
A document $d \in D$ is then preprocessed as follows by a PoS-Tagger:

\begin{equation*}
\begin{split}
	\mli{pos} &:  \mathcal{D} \to \mathcal{D} \\
	d &= (w_1,w_2,\ldots,w_{|d|}) \in \mathcal{D} \\
	\mli{pos}(d) &=  \left((w_1\underbrace{,
	m_1^1, \ldots,m_1^l}_{\mathrm{Modifiers}}), \ldots,
	(w_{|d|},m_{|d|}^1,\ldots,m_{|d|}^l)\right) \\ 
\end{split}
\end{equation*}

$V_P$ denotes the set of token-modifiers tuples by which
the vocabulary is extended after the tagging process. 

\begin{equation*}
V_P = \left\{ p_i^k \mid pos(d_i) = (p_1^i,\ldots,p_{|d|}^i), d_i \in D, 1 \leq
k \leq |d_i| \right\}
\end{equation*}
As with $N$-Grams, usually
the PoS-Tagged document is concantenated with original document and then
represented as BoW.

\subsection{Challenges in Text Classification}

The complexity and variability of human language makes text
classification a non-trivial task as it implies working in very high
dimensional feature spaces. Even for small datasets, with a few hundred of
samples, the BoW representation of a document can reach thousands of dimensions.
Adding complex linguistic features increases the dimensionality even more. The
problems resulting from high dimensional feature spaces are described in the
following sections.

\subsubsection{Sparse Feature Vectors}

In \cite{wang2012baselines} Wang et al. experimentally showed that just by
affixing bigrams as described in Section \ref{sssec:n-grams}, classification
accuracies can be slightly improved.
Nonetheless, using $N$-\textit{grams} with $N > 2$ didn't make the
classification accuracy better.
One way to explain why this happens, is by considering how the
statistics of each $N$-gram are affected, when $N$ grows.
 
Assume a unigram $v$ appears $N_v$ times in a given dataset $D$. All bigrams
$(v,v')$ for $v' \neq v$ can't possibly occur more often than  $v$ itself.
Generally speaking $N_{(v,v')} \leq N_v$. 
The same logic can be applied to $N$ and $N+1$ for $N >= 2$.\footnote{W.l.o.g.
this can also be said about bigrams $(v'',v)$ with $v '' \neq v$.} In the
extreme case, when $N = \max_{d' \in D} |d'|$, then every $N$-gram occurs at most one time. In short,
what we observe is that by building bigger and bigger $N$-grams the
Bag-Of-Words represantion of documents $d \in D$ becomes more sparse, i.e. in
proportion the number of zero-entries in a feature vector grows. 

For count-based classifiers, this this might present a problem. In its
frequentist interpretation, the probability of an event, in this case that an 
$N$-gram $v_k$ appears in class $\omega$, is approximated by
its relative frequency with respect to the counts of $v_k$ in the whole dataset: 

\begin{equation}
	\Pr(w|\omega) \approx \frac{N_{k,\omega}}{N_k} =:
	\hat{P}(w|\omega)
\end{equation}

The main claim of frequentist probability theory is that for inifnite samples
the relative frequency converges to the true underlying probability, in the same 
way as the relative frequency of throwing ``heads'' with a fair coin, will
approach 0.5 with a growing number of coin throws. Therefore, as we consider the
number of observed documents $|D|$ approaching infinity, we obtain 

\begin{equation} 
	\Pr(w|\omega) = \lim_{|D| \to \infty} \frac{N_{k,\omega}}{N_k}
\end{equation}

With fewer occurences for each $N$-gram, the estimates for the word probabilities
become more biased due to the scarcity of observations. This phenomenon is also
known as \emph{sampling error}. \footnote{If we assume a
gaussian distribution, then the sampling error $\epsilon \in
\mathcal{O}(1/\sqrt{|D|})$, which implies that in order to halve the error one
needs to enlarge the training set from $|D|$ to $|D|^2$ samples.} This error
propagates to the estimated posterior probability $\hat{P}(\omega|d)$.
The result is that words with low counts act like noise, which on average has no
impact on classification accuracy at all or can even worsen a classifier's
predictive ability. As a consequence, even though they contain more
information, arbitrarily adding higher order features such as $N$-grams or
PoS-tags cannot be expected to be generally beneficial to text classification
tasks. \footnote{What we here describe as statistical sparsity or sampling
error can be also viewed as a special case of the \emph{curse of
dimensionality}}

\subsubsection{Out of Vocabulary Words}
\label{ssec:oov}

Another challenge in text classification is \emph{Out-Of-Vocabulary
(OOV)} words. OOV words are encountered in the case in which an unknown document
$\tilde{d} = (\tilde{w}_1, \ldots, \tilde{w}_{|\tilde{d}|}) \in
\mathcal{D}\setminus D$ , i.e.
a document for which the true label is not known, contains words that are not
part of the classifier's known vocabulary.
As a consequence, the information provided by OOV words cannot be used for
prediction, since they do not have any representation in the classifier's feature 
space. Formally these are the words

\begin{equation*}
\tilde{V} \coloneqq \mathcal{V} \setminus V
\end{equation*}

Note that, for the same reasons described in the previous section, when using
complex higher features, OOV-words become even more frequent. 

\section{Case Study: Echobot}
\label{sec:echobot}
An example of the importance and at the same time the difficulties of text
classification can be found at \textit{Echobot}, a small
German company that develops machine learning based tools for social media
monitoring and sales. Their goal is to offer their customers solutions that
help oversee what the web is reporting about their products, their firm or
their competitors, helping them to adjust their business strategy according 
to trends detected on the web. Web content is retrieved by a complex web crawler
that scans millions of social media postings and hundreds of thousands
of articles and company websites daily. The information gathered in this way is
analyzed, structured and categorized with machine learning based software that
extracts useful and relevant information for the customer.

\subsection{Business Signals}

In one specific context, text classification is used to alert customers when
newly published web content concerns a specific topic. E.g. one might
be interested in knowing when a company of a specific industry is about to change
their management personell or has announced a site closure. 
Events like this, also called \textit{business signals}, are of particular
commercial interest for the customer. It is e.g. known that a restructuring 
of a company's managing personell, is often followed by the purchase of new
services and products. With this knowledge at hand, a salesman that has been
alerted of such event, could contact the company in question and make an offer
before anyone else.

\subsection{Semi-Automated Classification}
 
When classifying business signals, Echobot faces the same problems 
mentioned in the introductory chapter: labeled data is scarce and costly to produce, 
but nonetheless crucial for effective text categorization. Therefore, in order
to optimize classification accuracy inspite of the scarcity of labeled data, Echobot developed 
a semi-automated classification process that involves a human agent. Instead of
building a classification model based on the raw textual data, a domain expert
preprocesses the documents by annotating each sample with semantic and
syntactic information that will help the classifier build a more precise
classification model. As a matter of fact, the benefits of this method can be
observed experimentally: compared with its unpreprocessed variant, classification with
the annotated dataset performs significantly better. Therefore, an analysis of
how the domain expert preprocesses the datasets, might give an intuition for
an \emph{automated} preprocessing method that improves classification
effectiveness.

\subsection{Manual preprocessing by a domain expert}

The linguistic skills of the domain expert are leveraged for the selection 
and combination of a pool of PoS-tags and $N$-grams. Instead of arbitrarily
adding $N$-grams and PoS-tags, that could potentially harm classification quality, 
the operator chooses carefully a subset of features that is meaningful for the
underlying classification task.

We can describe the operations of the domain expert formally as follows:
let $D$ be a set of documents containing $N$-grams $V_N^*$ and
PoS-Tags $V_P$ such that $V = V_N^* \cup V_P$. Based on this vocabulary,
the resulting BoW feature vector has dimension $|V| = |V_P| + |V_N^*|$.
The operator's task is to combine the available $N$-grams and PoS-Tags
such that the resulting feature space reflects his understanding
of semantics for the underlying classification task.
More formally, this implies the creation of a list of rules
$\boldsymbol{\rho} = (\rho_1, \rho_2, \ldots, \rho_n)$, where each rule $\rho_i:
\mathcal{B} \to \mathbb{N}$ for $i=1,2,3,\ldots,n$ maps a subspace of the BoW
feature space $\mathcal{B}$ to a scalar value.

\begin{eqnarray*}
\mathcal{H} \coloneqq \mathbb{N}^n \\
\boldsymbol\rho: \mathcal{B} \to \mathcal{H} \\
\boldsymbol{\rho}(\mathbf{b}) = (\rho_1(\mathbf{b}), \ldots,
\rho_n(\mathbf{b}))^\intercal 
\end{eqnarray*}

In other words, after applying $\boldsymbol\rho$, the classification problem is
transfered from the large feature space of BoW's to a much lower dimensional and
semantically enriched vector space engineered by the domain expert.

The following toy example illustrates how such a rule might look
like for a specific vocabulary: 

\begin{eqnarray*}
V_U \supset \{\text{'good', 'not', bad'}\}, V_P \supset \{\text{``not'' - particle -
negates\ ``bad"'}\} \\
\rho_1(\mathbf{b}) = \mathbf{b}(\text{'good'}) \lor \left[\mathbf{b}(\text{'bad'})
\land \mathbf{b}(\text{'``not'' - particle - negates\ ``bad"'}\right]
\end{eqnarray*}

The rule above merges the words ``good'' and ``not bad'' to one new feature.
This is done by checking whether the word ``bad'' appears in combination with a
``not"-particle that negates it. Note that the same result could be achieved
by using the bigram ``not bad'' and merging it with the unigram ``good''.
In a task of \emph{sentiment analysis} this would be a sensible rule to
distinguish whether the word ``bad'' was used in a negative or in a positive
way. This cannot be achieved if the words ``not''
and ``bad'' are represented separately as unigrams, as the word ``not'' could a
appear in a context that does not refer to ``bad''.
Also, since the occurences of the term ``good'' and ``not bad'' are treated
equally, their statistics in the engineered feature space are combined and
hence will be more robust towards sampling errors.

In order to handle OOV words, the domain expert preemptively creates a set of
synonyms to which the classifier can fall back to in case a word is not contained 
in the known vocabulary. He thereby selects a subset of words $v^* \in V^*
\subset V$, that might be important for classification. Subsequently,
for each $v^* \in V^*$ a set $T_{v^*}$ of OOV words, that are semantically
similar to $v*$ is created.  Unknown samples $\tilde{d} \in D$ can then be
preprocessed as follows:

\begin{eqnarray*}
\tilde{d} := (\tilde{w_1},\ldots,\tilde{w}_{|\tilde{d}|}) \in
\mathcal{D}\setminus D
\\
\boldsymbol\pi: \mathcal{D} \to \mathcal{D} \\
\boldsymbol\pi(\tilde{d}) =
(\pi(\tilde{w_1}),\ldots,\pi(\tilde{w}_{|\tilde{d}|})) \\
\pi(\tilde{w}_k) = \begin{cases} \tilde{w}_k & \tilde{w}_k \in V  \\
v^* & \tilde{w}_k \in T_{v^*} \\ 
\mathrm{<unknown>} & \mathrm{otherwise} \end{cases}& ,~k=1,\ldots,|\tilde{d}| 
\end{eqnarray*}

Where ``<unknown>'' is used as a token in case there is no alternative available
for an OOV word. 

Using the feature space generated by $\boldsymbol\rho$ is an implicit way
of removing the statistical sparsity when implying higher order features. 
(see Section \ref{sec:statistics}). 
On the other hand, $\boldsymbol\pi$ makes sure that the information provided by
OOV words can be used for classification, by mapping them onto the known
vocabulary $V$.

\subsection{Automating the manual preprocessing step}

It is noteworthy that in Echobot's productive environment the
list of rules  $\boldsymbol\rho$ and the OOV-word replacement function
$\boldsymbol\pi$ often consist of merging or substituting semantically similar
expressions and phrases.
As already discussed in Section \ref{sec:introduction:proposed-method}, word
embeddings offer the possibility to measure the semantic similarity between
words by just calculating the angle of the respective vectors in the embedding
vector space. With this tool at hand, an intuitevly sensible approach would be
to leverage the distance measure in combination with our novel distributional
similarity (see Section \ref{sec:contributions}) to \emph{automatically} find
suitable term substitutions. 

Given a dataset $D$ and a word embedding $\mathcal{W}$, automating the
substitution process implies solving the following two tasks:

\begin{enumerate}
  \item Find a function $\boldsymbol\sigma(D,\mathcal{W},\Theta_\sigma):
	\mathcal{B} \to \mathcal{H'}$, such that, given parameters $\Theta_\sigma$,
	documents are mapped from the high dimensional BoW feature space $\mathcal{B}$ 
	to a lower dimensional feature space $\mathcal{H'}$ that incorporates
	the semantic knowledge introduced by word embeddings and the implicit
	distributional information given by the dataset itself.

  \item Find a function $\tau(D,\mathcal{W}, \Theta_\tau): \tilde{V} \to V$ that
  maps OOV words $\tilde{V}$  to the known vocabulary $V$, given parameters
  $\Theta_\tau$.
  
\end{enumerate}

Coarsely speaking, $\boldsymbol\sigma$ acts as the automated counterpart  of the
rule set $\boldsymbol\rho$, whereas $\tau$ fulfills the role of the substitutions
$\pi$.

In the course of this work, we will show that $\boldsymbol\sigma$ and $\tau$ can
be realized by finding minimum weight cliques in the fully connected 
undirected graph $G=(V,V\times V,\mathbf{W})$, with nodes $V$ and weights
$\mathbf{W}$.
The nodes of $G$ are the words $v \in V$, wherase the weights
$\mathbf{W}$ are defined by the pairwise distances of $v\neq v' \in V$ given by
our novel metric, which incorporates both semantic and distributional
word similarity.
